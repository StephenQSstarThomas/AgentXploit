"""
Vulnerability analysis tool for identifying security issues in target agent code.
"""
import logging
from typing import List, Dict, Optional, Any

logger = logging.getLogger(__name__)

try:
    from google.adk.tools import ToolContext
except ImportError:
    ToolContext = Any


def analyze_vulnerability(
    tool_name: str,
    tool_description: str,
    data_sources: List[str],
    data_destinations: List[str],
    position: str,
    sanitization_present: bool = False,
    tool_context: Optional[ToolContext] = None
) -> dict:
    """Analyze potential vulnerabilities in an agent tool or data flow.

    Args:
        tool_name: Name of the tool being analyzed
        tool_description: Brief description of what the tool does
        data_sources: List of data sources (e.g., ["user_input", "web_content", "file_read"])
        data_destinations: List of where data flows to (e.g., ["llm_prompt", "bash_command", "file_write"])
        position: Location in code (e.g., "tools/file_reader.py:read_file_tool")
        sanitization_present: Whether any input validation/sanitization exists
        tool_context: ADK tool context (auto-injected)

    Returns:
        dict: {
            "tool_name": str,
            "position": str,
            "vulnerabilities": List[str],
            "injection_vectors": List[dict],
            "threat_model": List[str]
        }
    """
    try:
        vulnerabilities = []
        injection_vectors = []
        threat_model = []

        # Check for direct web-to-LLM flow
        if any("web" in src.lower() for src in data_sources):
            if any("llm" in dest.lower() or "prompt" in dest.lower() for dest in data_destinations):
                if not sanitization_present:
                    vulnerabilities.append("Direct web-to-LLM data flow without sanitization")
                    injection_vectors.append({
                        "type": "web_content_injection",
                        "source": "web_content",
                        "destination": "llm_prompt"
                    })
                    threat_model.append("prompt_injection")

        # Check for user input to bash execution
        if any("user" in src.lower() or "input" in src.lower() for src in data_sources):
            if any("bash" in dest.lower() or "command" in dest.lower() or "exec" in dest.lower() for dest in data_destinations):
                if not sanitization_present:
                    vulnerabilities.append("User input flows directly to command execution")
                    injection_vectors.append({
                        "type": "command_injection",
                        "source": "user_input",
                        "destination": "bash_execution"
                    })
                    threat_model.append("command_injection")

        # Check for file operations with external input
        if any("user" in src.lower() or "web" in src.lower() or "external" in src.lower() for src in data_sources):
            if any("file" in dest.lower() for dest in data_destinations):
                vulnerabilities.append("External data used in file operations")
                injection_vectors.append({
                    "type": "path_traversal",
                    "source": "external_input",
                    "destination": "file_operations"
                })
                threat_model.append("path_traversal")

        # Check for LLM output to privileged operations
        if any("llm" in src.lower() or "agent" in src.lower() for src in data_sources):
            if any("bash" in dest.lower() or "exec" in dest.lower() for dest in data_destinations):
                vulnerabilities.append("LLM output directly controls privileged operations")
                injection_vectors.append({
                    "type": "prompt_to_execution",
                    "source": "llm_output",
                    "destination": "privileged_operation"
                })
                threat_model.append("prompt_injection")

        result = {
            "tool_name": tool_name,
            "position": position,
            "vulnerabilities": vulnerabilities if vulnerabilities else [],
            "injection_vectors": injection_vectors,
            "threat_model": list(set(threat_model))  # Deduplicate
        }

        # Store in context if available
        if tool_context is not None:
            vulnerabilities_key = "analysis:vulnerabilities"
            existing_vulns = tool_context.state.get(vulnerabilities_key, [])
            existing_vulns.append(result)
            tool_context.state[vulnerabilities_key] = existing_vulns

        logger.info(f"Analyzed: {tool_name}")
        return result

    except Exception as e:
        logger.error(f"Analysis error: {e}")
        return {
            "tool_name": tool_name,
            "position": position,
            "vulnerabilities": [f"Analysis error: {str(e)}"],
            "injection_vectors": [],
            "threat_model": []
        }


__all__ = ["analyze_vulnerability"]
