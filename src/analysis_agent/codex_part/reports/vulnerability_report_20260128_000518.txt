Traditional Security Vulnerability Assessment
Target: /home/shiqiu/gpt_academic-3.90
Date: 2026-01-28

Summary
- Primary language: Python
- Frameworks/server stack: Gradio + FastAPI + Uvicorn (see shared_utils/fastapi_server.py)
- Entry points: main.py (Gradio app), shared_utils/fastapi_server.py (FastAPI mount), multiple plugins under crazy_functions/
- High-risk subsystems: file upload + archive extraction, dynamic code generation/execution plugins, configuration mutation via UI

Methodology (Coverage-first, then depth)
- Pass A (broad search): rg for subprocess, eval/exec, pickle, archive extraction, file I/O, auth, network calls
- Pass B (sink-centric): reviewed command execution, deserialization, archive extraction, file write/download logic
- Pass C (boundary-centric): traced HTTP/upload entry points, config mutation path, plugin execution boundaries

Findings

1) Arbitrary Code Execution via “函数动态生成” plugin (Dynamic Code Execution)
Severity: Critical
Justification: Untrusted code is written to disk and imported/executed server-side. Any user who can access this plugin can execute arbitrary Python with server privileges.
Location:
- /home/shiqiu/gpt_academic-3.90/crazy_functions/函数动态生成.py:200-231
- /home/shiqiu/gpt_academic-3.90/crazy_functions/gen_fns/gen_fns_shared.py:14-68
Evidence (minimal snippet):
- /home/shiqiu/gpt_academic-3.90/crazy_functions/函数动态生成.py:226-230
  p = multiprocessing.Process(target=subprocess_worker, args=(code, file_path, return_dict))
  p.start(); p.join(timeout=TIME_LIMIT)
- /home/shiqiu/gpt_academic-3.90/crazy_functions/gen_fns/gen_fns_shared.py:54-67
  fn_path = f'{get_log_folder(plugin_name="gen_plugin_run")}/{module_file}.py'
  with open(fn_path, 'w', encoding='utf8') as f: f.write(code)
  module_spec = importlib.util.spec_from_file_location('example_module', fn_path)
  example_module = importlib.util.module_from_spec(module_spec)
  module_spec.loader.exec_module(example_module)
  instance = some_class()
  return_dict['result'] = instance.run(file_path)
Attack scenario:
- An attacker uses the “函数动态生成” plugin and provides prompts that cause the model to emit malicious Python (e.g., os.system, reverse shell). The code is written to disk and executed in a subprocess without sandboxing.
Impact:
- Remote code execution, data theft, persistence, lateral movement, full host compromise depending on server privileges.
Preconditions/assumptions:
- The plugin is enabled and reachable by the attacker (direct UI access or via “虚空终端” plugin routing).
Remediation:
- Preferred fix: Remove dynamic code execution or restrict to a safe, pre-reviewed allowlist of functions; do not import/exec untrusted code.
- Defense-in-depth: Execute in a hardened sandbox (container/jail), drop privileges, seccomp/AppArmor, read-only FS, strict network egress rules, time/memory limits, and disable access to sensitive paths.
- Secure-by-default alternative: Provide a fixed set of vetted transformations instead of free-form code generation; if extensibility is needed, use signed plugins and a review pipeline.

2) Arbitrary Code Execution via “数学动画生成manim” plugin (Untrusted Code Execution)
Severity: Critical
Justification: Generated code from untrusted input is written to disk and executed by Python without sandboxing.
Location:
- /home/shiqiu/gpt_academic-3.90/crazy_functions/数学动画生成manim.py:17-34, 95-98
Evidence (minimal snippet):
- /home/shiqiu/gpt_academic-3.90/crazy_functions/数学动画生成manim.py:20-34
  with open('gpt_log/MyAnimation.py', 'w', encoding='utf8') as f:
      f.write(code)
  subprocess.check_output([sys.executable, '-c', f"from gpt_log.MyAnimation import {class_name}; {class_name}().render()"]) 
Attack scenario:
- An attacker submits a prompt that produces Python code with malicious side effects. The system writes it to gpt_log/MyAnimation.py and executes it, giving the attacker code execution.
Impact:
- Same as above: RCE, data exfiltration, persistence.
Preconditions/assumptions:
- Plugin is enabled and accessible to the attacker.
Remediation:
- Preferred fix: Eliminate executing generated code; replace with a safe render service or a vetted template system.
- Defense-in-depth: Run in a locked-down container, no filesystem access except a scratch dir, no network, strict CPU/memory limits, and revoke host file access.
- Secure-by-default alternative: Use a separate, isolated render service with authentication and strict input validation.

3) Remote Code Execution via eval() on user-controlled configuration values (Unsafe Evaluation)
Severity: High
Justification: Untrusted user input can reach eval() when configuration is modified at runtime. If ALLOW_RESET_CONFIG is enabled, a remote user can trigger arbitrary code execution by setting config values that parse as dict/list.
Location:
- /home/shiqiu/gpt_academic-3.90/shared_utils/config_loader.py:45-53, 120-125
- /home/shiqiu/gpt_academic-3.90/crazy_functions/vt_fns/vt_modify_config.py:9-58
Evidence (minimal snippet):
- /home/shiqiu/gpt_academic-3.90/shared_utils/config_loader.py:46-52
  elif isinstance(default_value, dict):
      r = eval(env_arg)
  elif isinstance(default_value, list):
      r = eval(env_arg)
  elif default_value is None:
      r = eval(env_arg)
- /home/shiqiu/gpt_academic-3.90/crazy_functions/vt_fns/vt_modify_config.py:56-58
  from toolbox import set_conf
  set_conf(explicit_conf, user_intention.new_option_value)
Attack scenario:
- With ALLOW_RESET_CONFIG=True, a user invokes “虚空终端” → ModifyConfiguration and sets a list/dict config (e.g., AUTHENTICATION or proxies) to a payload like "__import__('os').system('id')". The value is stored in the environment and later eval() is called, executing arbitrary code.
Impact:
- RCE in the main server process, credential theft, data loss, full host compromise.
Preconditions/assumptions:
- ALLOW_RESET_CONFIG is enabled and the attacker can access the ModifyConfiguration path.
Remediation:
- Preferred fix: Replace eval() with safe parsers (json.loads for dict/list; ast.literal_eval at most) and validate schema/types strictly.
- Defense-in-depth: Disable runtime config mutation by default; require admin authentication and a restricted allowlist of modifiable keys.
- Secure-by-default alternative: Use pydantic models and explicit JSON-only config updates.

4) Path Traversal via Archive Extraction (Tar Symlink/Hardlink write-outside) (Arbitrary File Write)
Severity: High
Justification: Tar archives can contain symlinks/hardlinks that bypass the current member.name validation, allowing files to be written outside dest_dir during extractall(). This enables overwriting sensitive files and can lead to RCE.
Location:
- /home/shiqiu/gpt_academic-3.90/shared_utils/handle_upload.py:91-116
- /home/shiqiu/gpt_academic-3.90/toolbox.py:498-526 (entry point for uploads)
Evidence (minimal snippet):
- /home/shiqiu/gpt_academic-3.90/shared_utils/handle_upload.py:106-116
  for member in tarobj.getmembers():
      member_path = os.path.normpath(member.name)
      full_path = os.path.join(dest_dir, member_path)
      full_path = os.path.abspath(full_path)
      if not full_path.startswith(os.path.abspath(dest_dir) + os.sep):
          raise Exception(...)
  tarobj.extractall(path=dest_dir)
Attack scenario:
- Attacker uploads a crafted .tar.gz with a symlink entry like “logs -> /etc” followed by “logs/cron.d/evil”. The member.name checks pass (within dest_dir), but extractall follows the symlink and writes outside the intended directory. This can overwrite configs or drop a malicious pickle for later execution.
Impact:
- Arbitrary file write outside upload directory; potential RCE, credential theft, or service tampering.
Preconditions/assumptions:
- Attacker can upload an archive via the UI (file upload path in toolbox.on_file_uploaded).
Remediation:
- Preferred fix: Implement safe extraction that explicitly blocks symlinks/hardlinks and validates final extraction paths. For tarfile, inspect member.issym()/islnk() and member.linkname; only allow regular files/dirs and ensure resolved path stays within dest_dir before writing.
- Defense-in-depth: Extract in a sandboxed temp directory with restricted permissions; enforce size limits and file count limits.
- Secure-by-default alternative: Use a vetted safe-extraction library or Python 3.11’s tarfile “filter='data'” safe extraction pattern plus explicit path checks.

5) Unsafe Deserialization with pickle.load from writable paths (Potential RCE)
Severity: Medium (Potential)
Justification: pickle.load executes arbitrary code during deserialization. If an attacker can write to these files (e.g., via upload traversal), they can achieve RCE when the plugins load the checkpoint/cache.
Location:
- /home/shiqiu/gpt_academic-3.90/crazy_functions/Social_Helper.py:40-45
- /home/shiqiu/gpt_academic-3.90/multi_language.py:118-121
Evidence (minimal snippet):
- /home/shiqiu/gpt_academic-3.90/crazy_functions/Social_Helper.py:43-45
  with open(os.path.join(checkpoint_dir, 'social_network.pkl'), "rb") as f:
      social_network = pickle.load(f)
- /home/shiqiu/gpt_academic-3.90/multi_language.py:118-120
  if cache_path is not None and os.path.exists(cache_path):
      with open(cache_path, "rb") as f:
          cache = pickle.load(f)
Attack scenario:
- Attacker writes a malicious pickle to the user’s log directory (directly or by abusing archive traversal). When the Social Helper plugin loads the checkpoint or multi_language cache, code executes.
Impact:
- RCE under the server’s privileges.
Preconditions/assumptions:
- Attacker can write to the log/cache path (e.g., via file upload traversal, compromised local account, or another file write bug).
Remediation:
- Preferred fix: Replace pickle with a safe serialization format (JSON, msgpack) and enforce schema validation.
- Defense-in-depth: Sign checkpoint files (HMAC) and verify before loading; isolate write permissions to a trusted service.
- Secure-by-default alternative: Use dataclass + json serialization with pydantic validation.

Notes on Testing / Reproduction
- Findings are code-grounded and can be reproduced by tracing the identified entry points. No services were started.
- High-priority remediation should focus on eliminating untrusted code execution paths and hardening archive extraction.
