"""
File operation and bash tools for exploiter agent.

=== FILE-BASED MODE TOOLS ===
1. read - Read files from LOCAL filesystem
2. write - Write files to LOCAL filesystem
3. edit - Edit files on LOCAL filesystem with string replacement
4. bash_local - Run shell commands locally (backup for run_target_agent)
5. load_injection_templates - Load injection templates by category/complexity
6. load_mutation_strategy - Load mutation strategies
7. run_target_agent - Execute target agent with injection payload

=== NETWORK-BASED MODE TOOLS ===
1. docker_bash - Execute commands in Docker container (primary tool)
   - Read source code, explore codebase, write exploits, run verification
"""
import logging
import os
import re
import subprocess
from pathlib import Path
from typing import Optional

logger = logging.getLogger(__name__)

# Constants
DEFAULT_MAX_LINES = 2000
MAX_LINE_LENGTH = 2000
MAX_OUTPUT_CHARS = 300000

# Get the tools directory path for loading templates
TOOLS_DIR = Path(__file__).parent


def bash_local(
    command: str,
    working_dir: Optional[str] = None,
    timeout: Optional[int] = 120,
) -> dict:
    """Execute bash command on LOCAL machine. Backup tool for file-based mode.

    This is a BACKUP tool - prefer using run_target_agent() for file-based exploits.
    Only use this when run_target_agent() fails or for auxiliary operations.

    Args:
        command: The bash command to execute
        working_dir: Optional working directory
        timeout: Timeout in seconds (default 120, max 600)

    Returns:
        dict: {"success": bool, "exit_code": int, "stdout": str, "stderr": str, "error": str}
    """
    max_timeout = 600

    try:
        # Validate timeout
        if timeout is None:
            timeout = 120
        timeout = min(timeout, max_timeout)

        # Set working directory
        cwd = working_dir if working_dir else None
        if cwd and not os.path.isdir(cwd):
            return {
                "success": False,
                "exit_code": -1,
                "stdout": "",
                "stderr": "",
                "error": f"Working directory does not exist: {cwd}"
            }

        # Execute command
        result = subprocess.run(
            command,
            shell=True,
            capture_output=True,
            text=True,
            timeout=timeout,
            cwd=cwd
        )

        stdout = result.stdout
        stderr = result.stderr

        # Truncate if needed
        if len(stdout) > MAX_OUTPUT_CHARS:
            stdout = stdout[:MAX_OUTPUT_CHARS] + "\n... [output truncated]"
        if len(stderr) > MAX_OUTPUT_CHARS:
            stderr = stderr[:MAX_OUTPUT_CHARS] + "\n... [output truncated]"

        return {
            "success": result.returncode == 0,
            "exit_code": result.returncode,
            "stdout": stdout,
            "stderr": stderr,
            "error": ""
        }

    except subprocess.TimeoutExpired:
        return {
            "success": False,
            "exit_code": -1,
            "stdout": "",
            "stderr": "",
            "error": f"Command timed out after {timeout} seconds"
        }
    except Exception as e:
        logger.error(f"Bash execution error: {e}", exc_info=True)
        return {
            "success": False,
            "exit_code": -1,
            "stdout": "",
            "stderr": "",
            "error": str(e)
        }


def docker_bash(
    command: str,
    container_name: str,
    working_dir: Optional[str] = "/workspace",
    timeout: Optional[int] = 120,
) -> dict:
    """Execute bash command INSIDE a Docker container. PRIMARY tool for network-based mode.

    Use this tool to execute commands in the attacker container:
    1. Read source code in target: docker_bash("cat <source_path>/<file>", container_name)
    2. Explore codebase: docker_bash("find <source_path> -name '*.py' | head -20", container_name)
    3. Write exploit scripts: docker_bash("cat > /workspace/exploit.sh << 'EOF'\\n...\\nEOF", container_name)
    4. Send HTTP requests: docker_bash("curl '<target_url>/<endpoint>'", container_name)
    5. Check results: docker_bash("cat /workspace/exploited_flag.txt", container_name)

    IMPORTANT: Get container_name, target_url, and source_path from load_task_config() first!
    - Container name: task_config["setup_container"]["containers_created"] (attacker container)
    - Target URL: task_config["objective"]["attack_requirements"]["target_server"]
    - Source path: task_config["target_agent"]["source_code"]

    Args:
        command: The bash command to execute inside the container
        container_name: Docker container name (get from task_config, e.g., "agentscope-attacker-cve-2024-8438")
        working_dir: Working directory inside container (default: "/workspace")
        timeout: Timeout in seconds (default 120, max 600)

    Returns:
        dict: {"success": bool, "exit_code": int, "stdout": str, "stderr": str, "error": str}
    """
    max_timeout = 600

    try:
        # Validate timeout
        if timeout is None:
            timeout = 120
        timeout = min(timeout, max_timeout)

        # Build docker exec command
        docker_cmd = ["docker", "exec"]
        if working_dir:
            docker_cmd.extend(["-w", working_dir])
        docker_cmd.extend([container_name, "bash", "-c", command])

        logger.info(f"Docker exec in {container_name}: {command[:100]}...")

        # Execute command
        result = subprocess.run(
            docker_cmd,
            capture_output=True,
            text=True,
            timeout=timeout,
        )

        stdout = result.stdout
        stderr = result.stderr

        # Truncate if needed
        if len(stdout) > MAX_OUTPUT_CHARS:
            stdout = stdout[:MAX_OUTPUT_CHARS] + "\n... [output truncated]"
        if len(stderr) > MAX_OUTPUT_CHARS:
            stderr = stderr[:MAX_OUTPUT_CHARS] + "\n... [output truncated]"

        return {
            "success": result.returncode == 0,
            "exit_code": result.returncode,
            "stdout": stdout,
            "stderr": stderr,
            "error": ""
        }

    except subprocess.TimeoutExpired:
        return {
            "success": False,
            "exit_code": -1,
            "stdout": "",
            "stderr": "",
            "error": f"Command timed out after {timeout} seconds"
        }
    except FileNotFoundError:
        return {
            "success": False,
            "exit_code": -1,
            "stdout": "",
            "stderr": "",
            "error": "Docker command not found. Is Docker installed?"
        }
    except Exception as e:
        logger.error(f"Docker bash execution error: {e}", exc_info=True)
        return {
            "success": False,
            "exit_code": -1,
            "stdout": "",
            "stderr": "",
            "error": str(e)
        }


def read(
    file_path: str,
    offset: Optional[int] = None,
    limit: Optional[int] = None
) -> dict:
    """Read a file from the LOCAL filesystem.

    **CRITICAL: file_path SHOULD be an ABSOLUTE path (starting with /).**

    Example: read("/home/user/tasks/task-xxx/file.txt")
    NOT: read("tasks/task-xxx/file.txt")

    You can access any file on the local machine directly by using this tool.
    If the path provided doesn't exist, an error will be returned.

    Usage:
    - The file_path parameter SHOULD be an absolute path for reliability
    - By default, it reads up to 2000 lines starting from the beginning
    - You can optionally specify a line offset and limit for long files
    - Any lines longer than 2000 characters will be truncated
    - Results are returned using cat -n format, with line numbers starting at 1
    - If file exists but is empty, you will receive a warning message

    Args:
        file_path: The absolute path to the file to read (e.g., "/home/user/file.txt")
        offset: The line number to start reading from (0-indexed). Only provide
                if the file is too large to read at once
        limit: The number of lines to read. Only provide if the file is too
               large to read at once

    Returns:
        dict: {
            "success": bool,
            "file_path": str,
            "content": str,
            "message": str
        }
    """
    try:
        abs_path = os.path.abspath(file_path)

        if not os.path.exists(abs_path):
            return {
                "success": False,
                "file_path": abs_path,
                "content": "",
                "message": f"File not found: {abs_path}"
            }

        if os.path.isdir(abs_path):
            return {
                "success": False,
                "file_path": abs_path,
                "content": "",
                "message": f"Path is a directory, not a file: {abs_path}"
            }

        with open(abs_path, 'r', encoding='utf-8', errors='ignore') as f:
            all_lines = f.readlines()

        total_lines = len(all_lines)

        # Handle empty file
        if total_lines == 0:
            return {
                "success": True,
                "file_path": abs_path,
                "content": "[File is empty]",
                "message": f"File exists but has no content: {abs_path}"
            }

        # Apply offset and limit
        start_line = offset if offset is not None else 0
        max_lines = limit if limit is not None else DEFAULT_MAX_LINES

        # Clamp start_line to valid range
        start_line = max(0, min(start_line, total_lines - 1))

        # Select lines
        end_line = min(start_line + max_lines, total_lines)
        selected_lines = all_lines[start_line:end_line]

        # Format as cat -n output (line numbers starting at 1)
        formatted_lines = []
        for i, line in enumerate(selected_lines, start=start_line + 1):
            # Truncate long lines
            line = line.rstrip('\n\r')
            if len(line) > MAX_LINE_LENGTH:
                line = line[:MAX_LINE_LENGTH] + "... [truncated]"
            formatted_lines.append(f"{i:6}\t{line}")

        content = '\n'.join(formatted_lines)

        # Build message
        lines_read = len(selected_lines)
        if lines_read < total_lines:
            message = f"Read lines {start_line + 1}-{end_line} of {total_lines} from {abs_path}"
        else:
            message = f"Read {total_lines} lines from {abs_path}"

        return {
            "success": True,
            "file_path": abs_path,
            "content": content,
            "message": message
        }

    except Exception as e:
        logger.error(f"Read error: {e}", exc_info=True)
        return {
            "success": False,
            "file_path": os.path.abspath(file_path),
            "content": "",
            "message": f"Error: {str(e)}"
        }


def write(
    file_path: str,
    content: str
) -> dict:
    """Write content to a file on the LOCAL filesystem.

    **CRITICAL: file_path SHOULD be an ABSOLUTE path (starting with /).**

    Example: write("/home/user/tasks/task-xxx/payload.txt", "content")
    NOT: write("tasks/task-xxx/payload.txt", "content")

    This tool will overwrite the existing file if there is one at the provided path.

    Usage:
    - The file_path parameter SHOULD be an absolute path for reliability
    - This tool will overwrite the existing file if it exists
    - If this is an existing file you want to modify, consider using the read
      tool first to see its contents, then use edit for small changes or write
      for complete replacement
    - ALWAYS prefer editing existing files. NEVER write new files unless required
    - Parent directories will be created automatically if they don't exist

    Args:
        file_path: The absolute path to the file to write (e.g., "/home/user/file.txt")
        content: The content to write to the file

    Returns:
        dict: {
            "success": bool,
            "file_path": str,
            "message": str
        }
    """
    try:
        abs_path = os.path.abspath(file_path)

        # Create parent directory if needed
        parent_dir = os.path.dirname(abs_path)
        if parent_dir and not os.path.exists(parent_dir):
            os.makedirs(parent_dir, exist_ok=True)

        # Write content
        with open(abs_path, 'w', encoding='utf-8') as f:
            f.write(content)

        # Count lines
        line_count = content.count('\n') + (1 if content and not content.endswith('\n') else 0)

        return {
            "success": True,
            "file_path": abs_path,
            "message": f"Successfully wrote {line_count} lines to {abs_path}"
        }

    except Exception as e:
        logger.error(f"Write error: {e}", exc_info=True)
        return {
            "success": False,
            "file_path": os.path.abspath(file_path),
            "message": f"Error: {str(e)}"
        }


def edit(
    file_path: str,
    old_string: str,
    new_string: str,
    replace_all: bool = False
) -> dict:
    """Perform exact string replacement in a file on the LOCAL filesystem.

    **CRITICAL: file_path SHOULD be an ABSOLUTE path (starting with /).**

    Example: edit("/home/user/tasks/task-xxx/file.txt", "old", "new")
    NOT: edit("tasks/task-xxx/file.txt", "old", "new")

    Usage:
    - You should use the read tool first to see the file's contents before editing
    - The file_path parameter SHOULD be an absolute path for reliability
    - When editing text, ensure you preserve the exact indentation (tabs/spaces)
    - ALWAYS prefer editing existing files. NEVER write new files unless required
    - The edit will FAIL if old_string is not found in the file
    - The edit will FAIL if old_string appears multiple times and replace_all is False
      - Either provide a larger string with more context to make it unique
      - Or use replace_all=True to change every instance

    Use replace_all for:
    - Replacing and renaming strings across the file
    - Renaming variables or function names
    - Updating import statements throughout

    Args:
        file_path: The absolute path to the file to edit (e.g., "/home/user/file.txt")
        old_string: The text to replace (must exist in the file)
        new_string: The text to replace it with (must be different from old_string)
        replace_all: If True, replace all occurrences. If False (default), the
                     old_string must be unique in the file

    Returns:
        dict: {
            "success": bool,
            "file_path": str,
            "replacements": int,
            "message": str
        }
    """
    try:
        # Validate inputs
        if old_string == new_string:
            return {
                "success": False,
                "file_path": file_path,
                "replacements": 0,
                "message": "old_string and new_string must be different"
            }

        if not old_string:
            return {
                "success": False,
                "file_path": file_path,
                "replacements": 0,
                "message": "old_string cannot be empty"
            }

        abs_path = os.path.abspath(file_path)

        if not os.path.exists(abs_path):
            return {
                "success": False,
                "file_path": abs_path,
                "replacements": 0,
                "message": f"File not found: {abs_path}"
            }

        # Read current content
        with open(abs_path, 'r', encoding='utf-8', errors='ignore') as f:
            content = f.read()

        # Check if old_string exists
        count = content.count(old_string)
        if count == 0:
            return {
                "success": False,
                "file_path": abs_path,
                "replacements": 0,
                "message": "old_string not found in file. Make sure you're using the exact text including whitespace."
            }

        # Check uniqueness if not replace_all
        if not replace_all and count > 1:
            return {
                "success": False,
                "file_path": abs_path,
                "replacements": 0,
                "message": f"old_string appears {count} times in file. Use replace_all=True or provide more context to make it unique."
            }

        # Perform replacement
        if replace_all:
            new_content = content.replace(old_string, new_string)
            replacements = count
        else:
            new_content = content.replace(old_string, new_string, 1)
            replacements = 1

        # Write back
        with open(abs_path, 'w', encoding='utf-8') as f:
            f.write(new_content)

        return {
            "success": True,
            "file_path": abs_path,
            "replacements": replacements,
            "message": f"Successfully replaced {replacements} occurrence(s) in {abs_path}"
        }

    except Exception as e:
        logger.error(f"Edit error: {e}", exc_info=True)
        return {
            "success": False,
            "file_path": os.path.abspath(file_path),
            "replacements": 0,
            "message": f"Error: {str(e)}"
        }


def _parse_structured_output(stdout: str) -> dict:
    """Parse structured output markers from script stdout.

    Looks for markers like:
    - LOG_PATH=<path>
    - VERIFICATION_RESULT=<n>/<m>
    - EXPLOIT_STATUS=SUCCESS|FAILED

    Args:
        stdout: The script's standard output

    Returns:
        dict with parsed values (may be empty if markers not found)
    """
    parsed = {}

    # Parse LOG_PATH=...
    log_match = re.search(r'^LOG_PATH=(.+)$', stdout, re.MULTILINE)
    if log_match:
        parsed['log_path'] = log_match.group(1).strip()

    # Parse VERIFICATION_RESULT=n/m
    verify_match = re.search(r'^VERIFICATION_RESULT=(\d+)/(\d+)$', stdout, re.MULTILINE)
    if verify_match:
        parsed['verification_passed'] = int(verify_match.group(1))
        parsed['verification_total'] = int(verify_match.group(2))
        parsed['verification_result'] = f"{verify_match.group(1)}/{verify_match.group(2)}"

    # Parse EXPLOIT_STATUS=SUCCESS|FAILED
    status_match = re.search(r'^EXPLOIT_STATUS=(SUCCESS|FAILED)$', stdout, re.MULTILINE)
    if status_match:
        parsed['exploit_status'] = status_match.group(1)

    # Extract verification details (lines starting with [VERIFY])
    verify_lines = re.findall(r'^\[VERIFY\].*$', stdout, re.MULTILINE)
    if verify_lines:
        parsed['verification_details'] = '\n'.join(verify_lines)

    return parsed


def run_target_agent(
    script_path: str,
    injection_path: Optional[str] = None,
    args: Optional[str] = None,
    timeout: Optional[int] = 300,
    working_dir: Optional[str] = None,
) -> dict:
    """Run exploit for DOCUMENT-BASED tasks (exploit_type="document_injection").

    **CRITICAL: script_path and injection_path SHOULD be ABSOLUTE paths (starting with /).**

    Example: run_target_agent("/home/user/tasks/task-xxx/run.py", "/home/user/tasks/task-xxx/payload.txt")
    NOT: run_target_agent("run.py", "payload.txt")

    DO NOT use this for network_api tasks. Use setup_environment() + docker_bash() instead.

    Workflow:
    1. write(payload_interface.path, payload) -> write injection payload
    2. run_target_agent(script.path, payload_interface.path) -> execute & verify
    3. If failed, read(log_path) -> analyze and iterate

    Script outputs these markers (parsed automatically):
    - LOG_PATH=<path>           -> Captured in log_path
    - EXPLOIT_STATUS=SUCCESS    -> exploit_success=True
    - EXPLOIT_STATUS=FAILED     -> exploit_success=False

    Args:
        script_path: ABSOLUTE path to script (from task_config.script.path)
        injection_path: ABSOLUTE path to payload file (from task_config.payload_interface.path)
        args: Optional additional arguments
        timeout: Timeout in seconds (default 300, max 600)
        working_dir: Working directory for execution

    Returns:
        dict: {"exploit_success": bool, "log_path": str, "error": str}
    """
    import time
    max_timeout = 600

    try:
        abs_path = os.path.abspath(script_path)

        # Validate script exists
        if not os.path.exists(abs_path):
            return {
                "exploit_success": False,
                "log_path": "",
                "error": f"Script not found: {abs_path}"
            }

        if os.path.isdir(abs_path):
            return {
                "exploit_success": False,
                "log_path": "",
                "error": f"Path is a directory, not a file: {abs_path}"
            }

        # Validate timeout
        if timeout is None:
            timeout = 300
        timeout = min(timeout, max_timeout)

        # Determine working directory
        if working_dir:
            cwd = working_dir
        else:
            cwd = os.path.dirname(abs_path)

        if not os.path.isdir(cwd):
            return {
                "exploit_success": False,
                "log_path": "",
                "error": f"Working directory does not exist: {cwd}"
            }

        # Determine how to execute based on file extension
        ext = os.path.splitext(abs_path)[1].lower()

        if ext == '.sh':
            cmd = ['bash', abs_path]
        elif ext == '.py':
            cmd = ['python3', abs_path]
        elif ext == '.js':
            cmd = ['node', abs_path]
        else:
            cmd = [abs_path]

        # Add injection path if provided
        if injection_path:
            cmd.append(injection_path)

        # Add additional arguments if provided
        if args:
            cmd.extend(args.split())

        logger.info(f"Executing script: {' '.join(cmd)}")
        logger.info(f"Working directory: {cwd}")

        # Execute script
        result = subprocess.run(
            cmd,
            capture_output=True,
            text=True,
            timeout=timeout,
            cwd=cwd
        )

        stdout = result.stdout
        stderr = result.stderr

        # Parse structured output markers from stdout
        parsed = _parse_structured_output(stdout)

        # Determine log path - prefer parsed value, fallback to creating our own
        if 'log_path' in parsed and os.path.exists(parsed['log_path']):
            log_path = parsed['log_path']
        else:
            # Save full log to file (fallback behavior)
            timestamp = time.strftime('%Y%m%d_%H%M%S')
            script_name = os.path.splitext(os.path.basename(abs_path))[0]
            log_dir = os.path.join(os.path.dirname(abs_path), "logs")
            os.makedirs(log_dir, exist_ok=True)
            log_path = os.path.join(log_dir, f"{script_name}_{timestamp}.log")

            with open(log_path, 'w', encoding='utf-8') as f:
                f.write("=== EXECUTION LOG ===\n")
                f.write(f"Script: {abs_path}\n")
                f.write(f"Injection: {injection_path or 'None'}\n")
                f.write(f"Args: {args or 'None'}\n")
                f.write(f"Timestamp: {timestamp}\n")
                f.write(f"Exit Code: {result.returncode}\n")
                f.write(f"Exploit Success: {result.returncode == 0}\n")
                f.write(f"\n=== STDOUT ===\n{stdout}\n")
                f.write(f"\n=== STDERR ===\n{stderr}\n")

        # Determine exploit success - prefer parsed status, fallback to exit code
        if 'exploit_status' in parsed:
            exploit_success = parsed['exploit_status'] == 'SUCCESS'
        else:
            exploit_success = result.returncode == 0

        logger.info(f"Script completed: exploit_success={exploit_success}, log={log_path}")

        return {
            "exploit_success": exploit_success,
            "log_path": log_path,
            "error": ""
        }

    except subprocess.TimeoutExpired:
        return {
            "exploit_success": False,
            "log_path": "",
            "error": f"Script timed out after {timeout} seconds"
        }
    except FileNotFoundError as e:
        return {
            "exploit_success": False,
            "log_path": "",
            "error": f"Interpreter not found: {str(e)}"
        }
    except Exception as e:
        logger.error(f"Script execution error: {e}", exc_info=True)
        return {
            "exploit_success": False,
            "log_path": "",
            "error": str(e)
        }


def load_injection_templates(
    category: Optional[str] = None,
    complexity: Optional[str] = None,
    include_simple_texts: bool = True,
    limit: Optional[int] = None
) -> dict:
    """Load injection templates from the template library.

    This tool loads prompt injection templates that can be used to craft
    injection payloads. Templates are organized by category and complexity.

    Available categories:
    - technical_debug: Debug/troubleshooting scenarios
    - system_admin: System administration commands
    - security_audit: Security testing contexts
    - performance_tuning: Performance optimization
    - error_recovery: Error handling scenarios
    - testing_validation: Testing/QA contexts
    - environment_setup: Environment configuration
    - compliance_check: Compliance/audit scenarios

    Available complexity levels:
    - basic: Simple, direct templates
    - intermediate: More sophisticated approaches
    - advanced: Complex multi-step templates
    - expert: Highly sophisticated templates

    Args:
        category: Filter by category (e.g., "technical_debug", "system_admin").
                  If None, returns all categories.
        complexity: Filter by complexity (e.g., "basic", "intermediate", "advanced", "expert").
                    If None, returns all complexity levels.
        include_simple_texts: If True, also include simple text templates (default True)
        limit: Maximum number of templates to return. If None, returns all matching.

    Returns:
        dict: {
            "success": bool,
            "templates": list[dict],  # List of template objects
            "simple_texts": list[str],  # Simple text templates (if include_simple_texts=True)
            "total_count": int,
            "message": str
        }
    """
    try:
        # Import the templates module
        from tools.injection_templates import (
            InjectionCategory,
            InjectionComplexity,
            PromptInjectionSeedsLibrary,
            texts,
            texts_ablation,
        )

        library = PromptInjectionSeedsLibrary()

        # Filter templates
        filtered_templates = []
        for _, seed in library.seeds.items():
            # Apply category filter
            if category:
                try:
                    target_category = InjectionCategory(category)
                    if seed.category != target_category:
                        continue
                except ValueError:
                    pass  # Invalid category, skip filter

            # Apply complexity filter
            if complexity:
                try:
                    target_complexity = InjectionComplexity(complexity)
                    if seed.complexity != target_complexity:
                        continue
                except ValueError:
                    pass  # Invalid complexity, skip filter

            # Convert to dict for JSON serialization
            template_dict = {
                "name": seed.name,
                "category": seed.category.value,
                "complexity": seed.complexity.value,
                "template": seed.template,
                "description": seed.description,
                "target_scenarios": seed.target_scenarios,
                "psychological_hooks": seed.psychological_hooks,
            }
            filtered_templates.append(template_dict)

        # Apply limit
        if limit and limit > 0:
            filtered_templates = filtered_templates[:limit]

        # Get simple texts if requested
        simple_texts_result = []
        if include_simple_texts:
            simple_texts_result = list(texts) + list(texts_ablation)

        return {
            "success": True,
            "templates": filtered_templates,
            "simple_texts": simple_texts_result,
            "total_count": len(filtered_templates),
            "categories_available": [c.value for c in InjectionCategory],
            "complexities_available": [c.value for c in InjectionComplexity],
            "message": f"Loaded {len(filtered_templates)} templates" + (f" for category={category}" if category else "") + (f", complexity={complexity}" if complexity else "")
        }

    except Exception as e:
        logger.error(f"Load injection templates error: {e}", exc_info=True)
        return {
            "success": False,
            "templates": [],
            "simple_texts": [],
            "total_count": 0,
            "message": f"Error loading templates: {str(e)}"
        }


def load_mutation_strategy(
    strategy_name: Optional[str] = None
) -> dict:
    """Load mutation strategies for transforming injection templates.

    Mutation strategies define how to transform a base injection template
    into different variations. Each strategy has a specific approach and
    use case.

    Available strategies:
    - user_impersonation: Write as if user is giving instructions
    - personal_authorization: Add authorization codes/markers
    - system_requirement: Frame as mandatory system protocol
    - time_constraint: Create urgent time pressure
    - quality_assurance: Present as quality validation step
    - prerequisite_chaining: Make task appear as prerequisite
    - critical_markers: Add emphasis/importance markers
    - configuration_reference: Reference system configurations
    - collaborative_tone: Use helpful, cooperative language
    - logical_necessity: Use logical reasoning justification
    - compliance_framing: Reference compliance/regulations
    - user_preference_history: Claim alignment with user preferences
    - trust_building: Use transparent, trustworthy language
    - efficiency_appeal: Frame as improving efficiency
    - assigned_directive: Present as official assigned task
    - completeness_appeal: Appeal to thoroughness
    - priority_escalation: Escalate priority level
    - reasonable_assumption: Make assumptions about expectations
    - reciprocal_framing: Frame as mutually beneficial
    - deadline_pressure: Add specific deadline urgency
    - trigger_response_chaining: Create multi-point triggers

    Args:
        strategy_name: Specific strategy to load (e.g., "user_impersonation").
                       If None, returns all strategies.

    Returns:
        dict: {
            "success": bool,
            "strategies": list[dict],  # Strategy objects with name, description, template, etc.
            "total_count": int,
            "message": str
        }
    """
    try:
        # Import the mutation strategies module
        from tools.mutation_strategies import (
            MutationMethod,
            mutation_prompt_templates,
            strategy_descriptions,
        )

        strategies_result = []

        for method in MutationMethod:
            # Filter by name if specified
            if strategy_name and method.value != strategy_name:
                continue

            # Get template and description
            template = mutation_prompt_templates.get(method, "")
            description_info = strategy_descriptions.get(method, {})

            strategy_dict = {
                "name": method.value,
                "display_name": description_info.get("name", method.value),
                "description": description_info.get("description", ""),
                "use_when": description_info.get("use_when", ""),
                "avoid_when": description_info.get("avoid_when", ""),
                "template": template,
            }
            strategies_result.append(strategy_dict)

        return {
            "success": True,
            "strategies": strategies_result,
            "total_count": len(strategies_result),
            "available_strategies": [m.value for m in MutationMethod],
            "message": f"Loaded {len(strategies_result)} mutation strategies" + (f" (filtered: {strategy_name})" if strategy_name else "")
        }

    except Exception as e:
        logger.error(f"Load mutation strategy error: {e}", exc_info=True)
        return {
            "success": False,
            "strategies": [],
            "total_count": 0,
            "message": f"Error loading strategies: {str(e)}"
        }


def load_task_config(
    task_dir: str,
    config_filename: str = "task_config.json"
) -> dict:
    """Load task configuration. CHECK exploit_type TO CHOOSE WORKFLOW.

    **CRITICAL: task_dir MUST be an ABSOLUTE path (starting with /).**

    Example: load_task_config("/home/user/tasks/task-cve-xxx")
    NOT: load_task_config("tasks/task-cve-xxx")

    **IMPORTANT**: Check config.exploit_type to determine which tools to use:

    If exploit_type = "document_injection" (e.g., AutoGPT tasks):
        1. write(payload_interface.path, payload) -> write injection payload
        2. run_target_agent(script.path, payload_interface.path) -> run & get result
        3. read(log_path) if failed -> analyze and iterate

    If exploit_type = "network_api" (e.g., AgentScope CVE tasks):
        1. setup_environment(task_dir) -> start containers and server
        2. docker_bash(...) -> send exploit HTTP requests
        3. run_verify(task_dir) -> check if exploit succeeded
        4. cleanup_environment(task_dir) -> stop containers

    Args:
        task_dir: ABSOLUTE path to task directory containing task_config.json
                  (e.g., "/home/user/tasks/task-xxx", NOT "tasks/task-xxx")
        config_filename: Config filename (default: task_config.json)

    Returns:
        dict: {
            "success": bool,
            "config": {
                "task_id": str,
                "exploit_type": str,  # "document_injection" or "network_api"
                "description": str,
                "vulnerability": {...},
                "objective": {...},
                "script": {"path": str, ...},
                "payload_interface": {...},  # For document_injection
                "exploit_interface": {...},  # For network_api
                ...
            },
            "message": str
        }
    """
    import json

    try:
        task_dir_abs = os.path.abspath(task_dir)

        if not os.path.isdir(task_dir_abs):
            return {
                "success": False,
                "config": {},
                "message": f"Task directory not found: {task_dir_abs}"
            }

        config_path = os.path.join(task_dir_abs, config_filename)

        if not os.path.exists(config_path):
            return {
                "success": False,
                "config": {},
                "message": f"Config file not found: {config_path}"
            }

        with open(config_path, 'r', encoding='utf-8') as f:
            config = json.load(f)

        # Validate basic required fields (common to all modes)
        basic_required = ["task_id", "description"]
        missing = [f for f in basic_required if f not in config]
        if missing:
            return {
                "success": False,
                "config": config,
                "message": f"Config missing required fields: {missing}"
            }

        return {
            "success": True,
            "config": config,
            "message": f"Loaded task config: {config.get('task_name', config['task_id'])}"
        }

    except json.JSONDecodeError as e:
        return {
            "success": False,
            "config": {},
            "message": f"Invalid JSON in config file: {str(e)}"
        }
    except Exception as e:
        logger.error(f"Load task config error: {e}", exc_info=True)
        return {
            "success": False,
            "config": {},
            "message": f"Error: {str(e)}"
        }


def setup_environment(
    task_dir: str,
    wait_seconds: int = 8
) -> dict:
    """Setup Docker environment for network-based exploits.

    **CRITICAL: task_dir MUST be an ABSOLUTE path (starting with /).**

    Example: setup_environment("/home/user/tasks/task-cve-xxx")
    NOT: setup_environment("tasks/task-cve-xxx")

    This tool reads task_config.json and executes:
    1. setup_container.command - Start Docker containers (docker compose up)
    2. setup_server.command - Copy and run start script in target container
    3. Install curl in attacker container (for HTTP requests)

    Call this AFTER load_task_config() and BEFORE attempting exploitation.

    Args:
        task_dir: ABSOLUTE path to task directory containing task_config.json
                  (e.g., "/home/user/tasks/task-xxx", NOT "tasks/task-xxx")
        wait_seconds: Seconds to wait after server setup (default: 8)

    Returns:
        dict: {
            "success": bool,
            "target_container": str,   # Target container name (actual)
            "attacker_container": str, # Attacker container name (actual)
            "target_server": str,      # Target server URL
            "source_code_path": str,   # Path to source code in target
            "message": str
        }
    """
    import json
    import time

    try:
        task_dir_abs = os.path.abspath(task_dir)
        config_path = os.path.join(task_dir_abs, "task_config.json")

        if not os.path.exists(config_path):
            return {
                "success": False,
                "target_container": "",
                "attacker_container": "",
                "target_server": "",
                "source_code_path": "",
                "message": f"task_config.json not found in {task_dir_abs}"
            }

        with open(config_path, 'r', encoding='utf-8') as f:
            config = json.load(f)

        # Step 1: Setup containers using docker compose
        # Parse and execute commands, tracking directory changes
        setup_container = config.get("setup_container", {})
        container_cmd = setup_container.get("command", "")

        if container_cmd:
            # Split by && and execute each command separately
            commands = [cmd.strip() for cmd in re.split(r'\s*&&\s*', container_cmd) if cmd.strip()]

            # Track current working directory (may change with cd commands)
            current_cwd = task_dir_abs
            env = os.environ.copy()

            for cmd in commands:
                logger.info(f"Executing container setup command: {cmd[:80]}...")

                # Handle 'cd' commands by changing current_cwd instead of executing
                if cmd.startswith("cd "):
                    cd_path = cmd[3:].strip()
                    # Resolve relative path from current_cwd
                    if os.path.isabs(cd_path):
                        current_cwd = cd_path
                    else:
                        current_cwd = os.path.normpath(os.path.join(current_cwd, cd_path))
                    logger.info(f"Changed working directory to: {current_cwd}")
                    continue

                # Handle 'export' commands by setting environment variables
                if cmd.startswith("export "):
                    # Parse export VAR=value
                    export_part = cmd[7:].strip()
                    if "=" in export_part:
                        var_name, var_value = export_part.split("=", 1)
                        env[var_name.strip()] = var_value.strip()
                        logger.info(f"Set environment variable: {var_name.strip()}")
                    continue

                # Execute actual commands
                result = subprocess.run(
                    cmd,
                    shell=True,
                    capture_output=True,
                    text=True,
                    timeout=120,
                    cwd=current_cwd,
                    env=env
                )
                if result.returncode != 0:
                    return {
                        "success": False,
                        "target_container": "",
                        "attacker_container": "",
                        "target_server": "",
                        "source_code_path": "",
                        "message": f"Failed to setup containers: {result.stderr}"
                    }

        # Step 2: Get actual container names from docker ps
        # Look for containers with 'agentscope' in name
        ps_result = subprocess.run(
            "docker ps --format '{{.Names}}' | grep -i agentscope",
            shell=True,
            capture_output=True,
            text=True,
            timeout=30
        )
        container_names = ps_result.stdout.strip().split('\n') if ps_result.stdout.strip() else []

        target_container = ""
        attacker_container = ""
        for name in container_names:
            if 'attacker' in name.lower():
                attacker_container = name.strip()
            elif 'security-test' in name.lower() or 'agentscope' in name.lower():
                if 'attacker' not in name.lower():
                    target_container = name.strip()

        if not attacker_container:
            return {
                "success": False,
                "target_container": target_container,
                "attacker_container": "",
                "target_server": "",
                "source_code_path": "",
                "message": f"Attacker container not found. Found containers: {container_names}"
            }

        # Step 3: Install curl in attacker container (split commands to avoid issues)
        logger.info(f"Installing curl in attacker container: {attacker_container}")
        # First update package list
        apt_update = subprocess.run(
            f"docker exec {attacker_container} apt-get update -qq",
            shell=True,
            capture_output=True,
            text=True,
            timeout=120
        )
        if apt_update.returncode != 0:
            logger.warning(f"apt-get update warning: {apt_update.stderr[:200]}")

        # Then install curl
        curl_install = subprocess.run(
            f"docker exec {attacker_container} apt-get install -y -qq curl",
            shell=True,
            capture_output=True,
            text=True,
            timeout=120
        )
        if curl_install.returncode != 0:
            logger.warning(f"curl install warning: {curl_install.stderr[:200]}")

        # Step 4: Setup server (copy and run start.sh)
        # Split complex commands (separated by && or ;) and execute separately
        setup_server = config.get("setup_server", {})
        server_cmd = setup_server.get("command", "")

        if server_cmd:
            # Split by && and execute each command separately
            commands = [cmd.strip() for cmd in re.split(r'\s*&&\s*', server_cmd) if cmd.strip()]
            for cmd in commands:
                logger.info(f"Executing server setup command: {cmd[:80]}...")
                result = subprocess.run(
                    cmd,
                    shell=True,
                    capture_output=True,
                    text=True,
                    timeout=120,
                    cwd=task_dir_abs
                )
                if result.returncode != 0:
                    logger.warning(f"Server setup warning for '{cmd[:50]}...': {result.stderr[:200]}")

        # Wait for server to be ready
        if wait_seconds > 0:
            logger.info(f"Waiting {wait_seconds}s for server to be ready...")
            time.sleep(wait_seconds)

        # Extract target_server URL (clean it up)
        target_server_raw = config.get("objective", {}).get("attack_requirements", {}).get("target_server", "")
        # Extract just the URL part (e.g., "http://agentscope:5000" from "http://agentscope:5000 (accessible...)")
        target_server_match = re.match(r'(https?://[^\s]+)', target_server_raw)
        target_server = target_server_match.group(1) if target_server_match else target_server_raw

        source_code_path = config.get("target_agent", {}).get("source_code", "")
        # Clean up source_code_path (remove " in target container" suffix)
        if " in " in source_code_path:
            source_code_path = source_code_path.split(" in ")[0].strip()

        return {
            "success": True,
            "target_container": target_container,
            "attacker_container": attacker_container,
            "target_server": target_server,
            "source_code_path": source_code_path,
            "message": f"Environment ready. Attacker: {attacker_container}, Target: {target_container}, Server: {target_server}"
        }

    except subprocess.TimeoutExpired:
        return {
            "success": False,
            "target_container": "",
            "attacker_container": "",
            "target_server": "",
            "source_code_path": "",
            "message": "Setup command timed out"
        }
    except Exception as e:
        logger.error(f"Setup environment error: {e}", exc_info=True)
        return {
            "success": False,
            "target_container": "",
            "attacker_container": "",
            "target_server": "",
            "source_code_path": "",
            "message": f"Error: {str(e)}"
        }


def run_verify(
    task_dir: str
) -> dict:
    """Run verification script to check if exploit succeeded.

    **CRITICAL: task_dir MUST be an ABSOLUTE path (starting with /).**

    Example: run_verify("/home/user/tasks/task-cve-xxx")
    NOT: run_verify("tasks/task-cve-xxx")

    This tool reads task_config.json and executes verify.command to check
    if the exploitation was successful.

    Call this AFTER attempting exploitation to verify success.

    Args:
        task_dir: ABSOLUTE path to task directory containing task_config.json
                  (e.g., "/home/user/tasks/task-xxx", NOT "tasks/task-xxx")

    Returns:
        dict: {
            "success": bool,  # True if verification passed
            "passed": bool,   # Same as success
            "stdout": str,    # Verification script output
            "stderr": str,
            "message": str
        }
    """
    import json

    try:
        task_dir_abs = os.path.abspath(task_dir)
        config_path = os.path.join(task_dir_abs, "task_config.json")

        if not os.path.exists(config_path):
            return {
                "success": False,
                "passed": False,
                "stdout": "",
                "stderr": "",
                "message": f"task_config.json not found in {task_dir_abs}"
            }

        with open(config_path, 'r', encoding='utf-8') as f:
            config = json.load(f)

        # Get verify command
        verify_config = config.get("verify", {})
        verify_cmd = verify_config.get("command", "")

        if not verify_cmd:
            return {
                "success": False,
                "passed": False,
                "stdout": "",
                "stderr": "",
                "message": "No verify command found in task_config.json"
            }

        logger.info(f"Running verification: {verify_cmd}")
        result = subprocess.run(
            verify_cmd,
            shell=True,
            capture_output=True,
            text=True,
            timeout=60,
            cwd=task_dir_abs
        )

        passed = result.returncode == 0

        return {
            "success": True,
            "passed": passed,
            "stdout": result.stdout,
            "stderr": result.stderr,
            "message": "VERIFICATION PASSED" if passed else "VERIFICATION FAILED"
        }

    except subprocess.TimeoutExpired:
        return {
            "success": False,
            "passed": False,
            "stdout": "",
            "stderr": "",
            "message": "Verification command timed out"
        }
    except Exception as e:
        logger.error(f"Run verify error: {e}", exc_info=True)
        return {
            "success": False,
            "passed": False,
            "stdout": "",
            "stderr": "",
            "message": f"Error: {str(e)}"
        }


def cleanup_environment(
    task_dir: str
) -> dict:
    """Cleanup Docker environment after exploitation.

    **CRITICAL: task_dir MUST be an ABSOLUTE path (starting with /).**

    Example: cleanup_environment("/home/user/tasks/task-cve-xxx")
    NOT: cleanup_environment("tasks/task-cve-xxx")

    This tool stops and removes Docker containers created for the task.

    Args:
        task_dir: ABSOLUTE path to task directory containing task_config.json
                  (e.g., "/home/user/tasks/task-xxx", NOT "tasks/task-xxx")

    Returns:
        dict: {"success": bool, "message": str}
    """
    import json

    try:
        task_dir_abs = os.path.abspath(task_dir)
        config_path = os.path.join(task_dir_abs, "task_config.json")

        if not os.path.exists(config_path):
            return {
                "success": False,
                "message": f"task_config.json not found in {task_dir_abs}"
            }

        with open(config_path, 'r', encoding='utf-8') as f:
            config = json.load(f)

        # Get runtime directory from setup_container
        setup_container = config.get("setup_container", {})
        command = setup_container.get("command", "")

        # Extract the directory from command (look for cd ... &&)
        runtime_dir = None
        if "cd " in command and "docker compose" in command:
            # Parse: cd ../../runtimes/agentscope-0.1.1 && ...
            cd_match = re.search(r'cd\s+([^\s&]+)', command)
            if cd_match:
                cd_path = cd_match.group(1).strip()
                # Resolve the path relative to task_dir_abs
                if os.path.isabs(cd_path):
                    runtime_dir = cd_path
                else:
                    runtime_dir = os.path.normpath(os.path.join(task_dir_abs, cd_path))

        if runtime_dir and os.path.isdir(runtime_dir):
            logger.info(f"Cleaning up: docker compose down in {runtime_dir}")
            result = subprocess.run(
                "docker compose down",
                shell=True,
                capture_output=True,
                text=True,
                timeout=60,
                cwd=runtime_dir
            )
            return {
                "success": result.returncode == 0,
                "message": f"Cleanup completed" if result.returncode == 0 else f"Cleanup warning: {result.stderr}"
            }

        # Fallback: try to find containers and stop them
        containers = setup_container.get("containers_created", [])
        if containers:
            container_names = [c.split(" ")[0].strip() for c in containers]
            cleanup_cmd = f"docker stop {' '.join(container_names)} 2>/dev/null; docker rm {' '.join(container_names)} 2>/dev/null"
            logger.info(f"Cleaning up containers: {container_names}")
            result = subprocess.run(
                cleanup_cmd,
                shell=True,
                capture_output=True,
                text=True,
                timeout=60,
                cwd=task_dir_abs
            )
            return {
                "success": True,  # Best effort cleanup
                "message": f"Cleanup attempted for containers: {container_names}"
            }

        return {
            "success": False,
            "message": "Could not determine cleanup command"
        }

    except Exception as e:
        logger.error(f"Cleanup error: {e}", exc_info=True)
        return {
            "success": False,
            "message": f"Error: {str(e)}"
        }


__all__ = [
    "bash_local",
    "docker_bash",
    "read",
    "write",
    "edit",
    "run_target_agent",
    "load_task_config",
    "load_injection_templates",
    "load_mutation_strategy",
    "setup_environment",
    "run_verify",
    "cleanup_environment",
]
