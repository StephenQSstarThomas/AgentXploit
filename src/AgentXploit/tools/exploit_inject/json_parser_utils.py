"""
Robust JSON parsing utilities for LLM responses

This module provides enhanced JSON parsing that handles:
1. ```json code block wrapping
2. Incomplete/truncated JSON responses 
3. Nested JSON objects
4. Fallback mechanisms for parsing failures
"""

import json
import re
import logging
from typing import Dict, Any, Optional

logger = logging.getLogger(__name__)


def robust_json_parse(response: str, context_name: str = "LLM response") -> Optional[Dict[str, Any]]:
    """
    Robust JSON parsing that handles various LLM response formats
    
    Args:
        response: The raw LLM response text
        context_name: Context for logging (e.g., "seed selection", "injection generation")
        
    Returns:
        Parsed JSON dict or None if all parsing attempts fail
    """
    if not response or not response.strip():
        logger.warning(f"Empty {context_name} - no content to parse")
        return None
        
    try:
        # Method 1: Direct JSON parsing
        data = json.loads(response)
        logger.debug(f"Successfully parsed {context_name} as direct JSON")
        return data
    except json.JSONDecodeError:
        pass
    
    # Method 2: Extract from ```json wrapper
    try:
        json_match = re.search(r'```json\s*\n?(.*?)\n?```', response, re.DOTALL)
        if json_match:
            json_str = json_match.group(1).strip()
            logger.debug(f"Extracted JSON from wrapper: {json_str[:200]}...")
            
            try:
                data = json.loads(json_str)
                logger.info(f"Successfully extracted {context_name} from ```json wrapper")
                return data
            except json.JSONDecodeError as parse_err:
                logger.warning(f"JSON wrapper content invalid for {context_name}: {parse_err}")
                logger.debug(f"Invalid JSON content: {json_str}")
    except Exception as e:
        logger.debug(f"JSON wrapper extraction failed for {context_name}: {e}")
    
    # Method 3: Find balanced JSON objects
    try:
        # Look for balanced braces starting with {
        json_match = re.search(r'\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\}', response, re.DOTALL)
        if json_match:
            json_str = json_match.group()
            logger.debug(f"Extracted JSON object: {json_str[:200]}...")
            
            try:
                data = json.loads(json_str)
                logger.info(f"Successfully extracted {context_name} from text")
                return data
            except json.JSONDecodeError as parse_err:
                logger.warning(f"Extracted JSON object invalid for {context_name}: {parse_err}")
    except Exception as e:
        logger.debug(f"JSON object extraction failed for {context_name}: {e}")
    
    # Method 4: Attempt to fix common truncation issues
    try:
        # Look for partial JSON that might be repairable
        partial_match = re.search(r'\{\s*"[^"]+"\s*:\s*[^,}]+', response)
        if partial_match:
            logger.warning(f"Found partial JSON in {context_name}, but unable to reconstruct")
            logger.debug(f"Partial content: {partial_match.group()}")
    except Exception as e:
        logger.debug(f"Partial JSON check failed for {context_name}: {e}")
    
    # Log detailed failure information
    logger.error(f"All JSON parsing methods failed for {context_name}")
    logger.error(f"Response length: {len(response)} characters")
    logger.error(f"Response starts with: {response[:100]}...")
    logger.error(f"Response ends with: ...{response[-100:]}")
    
    return None


def improve_llm_json_reliability(max_tokens: int = 1000, temperature: float = 0.3) -> Dict[str, Any]:
    """
    Get improved LLM parameters for more reliable JSON responses
    
    Args:
        max_tokens: Base max tokens (will be adjusted if needed)
        temperature: Base temperature (will be lowered for reliability)
        
    Returns:
        Dict with optimized LLM parameters compatible with LLMClient.call_llm()
    """
    # Ensure sufficient tokens for complete JSON responses
    adjusted_max_tokens = max(max_tokens, 1500)  # Minimum 1500 tokens
    
    # Lower temperature for more structured responses
    adjusted_temperature = min(temperature, 0.4)  # Maximum 0.4 temperature
    
    # Only return parameters that LLMClient.call_llm() accepts
    return {
        "max_tokens": adjusted_max_tokens,
        "temperature": adjusted_temperature,
        # Note: frequency_penalty and presence_penalty removed as they're not supported by LLMClient
    }


def create_json_prompt_instruction(expected_format: str) -> str:
    """
    Create standardized JSON format instruction for LLM prompts
    
    Args:
        expected_format: Description of expected JSON structure
        
    Returns:
        Formatted instruction text
    """
    return f"""
IMPORTANT: Respond with valid JSON only. Structure: {expected_format}

Requirements:
- Use valid JSON syntax with proper quotes and commas
- Complete all fields as specified
- Do not include any text outside the JSON structure
- Ensure the response is complete and not truncated
"""


# Export utilities
__all__ = [
    'robust_json_parse',
    'improve_llm_json_reliability', 
    'create_json_prompt_instruction'
]