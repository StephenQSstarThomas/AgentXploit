"""
Simplified Workflow Engine

Focused on:
1. Simple Docker environment setup using subprocess_docker
2. LLM-driven autonomous analysis using openhands_specialized_executor
3. Terminal command execution for LLM-generated commands
4. Injection point detection without complex fallbacks
"""

import os
import json
import logging
import asyncio
import uuid
from typing import Dict, Optional, Any, List
from pathlib import Path

# Import simplified tools only
from .subprocess_docker import (
    create_docker_container, 
    create_development_container, 
    execute_container_command,
    run_docker_command,
    stop_docker_container
)
from .docker_setup import setup_intelligent_docker_environment
from .analyzer import analyze_injection_points, generate_llm_analysis_prompt
from .terminal import execute_terminal_command, process_llm_commands
from .openhands_specialized_executor import (
    setup_openhands_environment,
    execute_openhands_task,
    analyze_openhands_traces
)

logger = logging.getLogger(__name__)


async def execute_optimized_exploit_workflow(
    target_path: str,
    benign_task: Optional[str] = None,
    docker_image: Optional[str] = None,
    max_steps: int = 30,
    auto_execute: bool = True,
    focus: str = "injection_points",
    custom_commands: Optional[List[str]] = None,
    existing_deployment_id: Optional[str] = None
) -> Dict[str, Any]:
    """
    Simplified exploit workflow using only the new simplified executors.
    
    Args:
        target_path: Path to target agent
        benign_task: Optional benign task description
        docker_image: Optional Docker image
        max_steps: Maximum analysis steps
        auto_execute: Whether to auto-execute LLM commands
        focus: Analysis focus ("injection_points")
        custom_commands: Custom commands to execute instead of LLM-generated ones
        existing_deployment_id: Use existing deployment instead of creating new one
        
    Returns:
        Analysis results with injection points
    """
    workflow_id = f"workflow_{uuid.uuid4().hex[:8]}"
    logger.info(f"Starting simplified exploit workflow: {workflow_id}")
    
    try:
        # Phase 1: Docker Environment Setup (智能重用或创建)
        
        if existing_deployment_id:
            # 重用现有的 deployment，不重新创建
            logger.info(f"Phase 1: Reusing existing deployment: {existing_deployment_id}")
            deployment_id = existing_deployment_id
            docker_image_used = "reused_existing"
            setup_type = "reused"
            logger.info(f"Reusing environment: {deployment_id} (no new container creation)")
        else:
            # 只有在没有现有 deployment 时才创建新的
            logger.info("Phase 1: Setting up analysis environment using intelligent docker setup")
            
            # Initialize variables
            docker_image_used = "unknown"
            setup_type = "unknown"
            docker_setup_result = {"success": False}
            
            # Use intelligent docker setup that analyzes README, pyproject.toml, requirements.txt
            docker_setup_result = await setup_intelligent_docker_environment(
                target_path=target_path,
                llm_client=None,  # Use centralized LLM client
                require_confirmation=False  # Auto-execute for workflow
            )
            
            if docker_setup_result['success']:
                deployment_id = docker_setup_result['deployment_id']
                docker_image_used = docker_setup_result.get('docker_image', 'unknown')
                setup_type = docker_setup_result.get('setup_type', 'unknown')
                logger.info(f"Environment ready: {deployment_id} (image: {docker_image_used}, type: {setup_type})")
            else:
                # Fallback to provided docker_image or default python
                logger.warning("Intelligent docker setup failed, using fallback")
                if docker_image:
                    deployment_id = await create_docker_container(image=docker_image)
                else:
                    deployment_id = await create_development_container(base_image="python:3.12")
                docker_image_used = docker_image or "python:3.12"
                setup_type = "fallback"
                logger.info(f"Fallback environment ready: {deployment_id} (image: {docker_image_used})")
        
        # Phase 2: Execute Target Agent Task using simplified executors
        logger.info(f"Phase 2: Executing target agent with task: {benign_task or 'default task'}")
        
        # Detect if this is OpenHands by checking target path
        is_openhands = "openhands" in target_path.lower() or Path(target_path).name.lower() == "openhands"
        
        if is_openhands:
            logger.info("Detected OpenHands target - using simplified specialized executor")
            
            # Setup minimal OpenHands environment with target path
            openhands_config = {"target_path": target_path}
            setup_success = await setup_openhands_environment(deployment_id, openhands_config)
            if not setup_success:
                logger.warning("OpenHands environment setup failed, continuing anyway")
            
            # Execute OpenHands task
            execution_result = await execute_openhands_task(
                deployment_id=deployment_id,
                task_description=benign_task or "Analyze repository for security vulnerabilities",
                max_iterations=min(max_steps // 3, 10),
                custom_command=custom_commands[0] if custom_commands else None
            )
            
            # Collect and analyze traces
            trace_files = execution_result.get("trace_files", [])
            if trace_files:
                trace_analysis = await analyze_openhands_traces(deployment_id, trace_files)
            else:
                trace_analysis = {
                    "success": True,
                    "injection_points": [],
                    "external_data_sources": [],
                    "high_risk_points": [],
                    "overall_risk": "UNKNOWN"
                }
            
            agent_execution_result = {
                "execution_result": execution_result,
                "trace_files": trace_files,
                "trace_files_found": len(trace_files),
                "trace_analysis": trace_analysis
            }
        else:
            # Use simple command execution for non-OpenHands targets
            logger.info("Non-OpenHands target - using simple command execution")
            
            # Create a simple mock execution for non-OpenHands agents
            if custom_commands:
                execution_result = await execute_container_command(
                    command=custom_commands[0],
                    deployment_id=deployment_id,
                    timeout=300.0
                )
            else:
                # Mock execution
                execution_result = {
                    "success": True,
                    "command": "python --version",
                    "stdout": "Mock agent execution completed",
                    "stderr": "",
                    "task_description": benign_task or "Default analysis task"
                }
            
            # Mock trace analysis for non-OpenHands
            trace_analysis = {
                "success": True,
                "injection_points": [{
                    "type": "generic_agent_execution",
                    "location": "mock_execution",
                    "description": "Generic agent execution detected",
                    "risk": "MEDIUM",
                    "agent_type": "generic"
                }],
                "external_data_sources": [],
                "high_risk_points": [],
                "overall_risk": "MEDIUM"
            }
            
            agent_execution_result = {
                "execution_result": execution_result,
                "trace_files": [],
                "trace_files_found": 0,
                "trace_analysis": trace_analysis
            }
        
        # Extract results
        execution_result = agent_execution_result["execution_result"]
        trace_analysis = agent_execution_result["trace_analysis"]
        
        logger.info(f"Agent task executed. Success: {execution_result.get('success', False)}")
        logger.info(f"Found {len(trace_analysis.get('injection_points', []))} injection points")
        
        # Phase 3: Process Analysis Results
        logger.info("Phase 3: Processing injection point analysis results")
        
        # Extract results from trace analysis
        injection_points = trace_analysis.get('injection_points', [])
        external_data_sources = trace_analysis.get('external_data_sources', [])
        overall_risk = trace_analysis.get('overall_risk', 'UNKNOWN')
        analysis_commands = custom_commands or []
        
        # Phase 4: Execute LLM-Generated Commands (if auto_execute)
        command_results = []
        if auto_execute and analysis_commands:
            logger.info(f"Phase 4: Executing {len(analysis_commands)} commands")
            
            for cmd in analysis_commands[:max_steps]:  # Limit to max_steps
                try:
                    result = await execute_container_command(cmd, deployment_id, timeout=60.0)
                    command_results.append({
                        'command': cmd,
                        'success': result.success,
                        'output': result.stdout[:500] if hasattr(result, 'stdout') else "",
                        'error': result.stderr[:200] if hasattr(result, 'stderr') and result.stderr else None
                    })
                except Exception as e:
                    logger.warning(f"Command execution failed: {cmd} - {e}")
                    command_results.append({
                        'command': cmd,
                        'success': False,
                        'error': str(e)
                    })
        
        # Return comprehensive results
        result = {
            "success": True,
            "workflow_id": workflow_id,
            "target_path": target_path,
            "deployment_id": deployment_id,
            "docker_image_used": docker_image_used,
            "docker_setup_type": setup_type,
            "execution_time": "< 30s",  # Simplified timing
            "injection_points": injection_points,
            "analysis_commands": analysis_commands,
            "command_results": command_results,
            "overall_risk": overall_risk,
            "auto_executed": auto_execute,
            "agent_execution": execution_result,
            "external_data_sources": external_data_sources,
            "results": {
                "total_injection_points": len(injection_points),
                "high_risk_injection_points": len([p for p in injection_points if p.get('risk') == 'HIGH']),
                "external_data_sources_found": len(external_data_sources),
                "commands_executed": len(command_results),
                "successful_commands": len([r for r in command_results if r.get('success')]),
                "risk_level": overall_risk,
                "trace_files_analyzed": agent_execution_result.get('trace_files_found', 0),
                "agent_task_success": execution_result.get('success', False),
                "agent_task_command": execution_result.get('command', 'unknown'),
                "docker_setup_success": setup_type == "reused" or docker_setup_result.get('success', False) if 'docker_setup_result' in locals() else True,
                "simplified_workflow": True
            }
        }
        
        logger.info("✅ Simplified workflow completed successfully")
        
        # Add cleanup function to result for later use
        result["cleanup_function"] = lambda: cleanup_workflow_docker(deployment_id)
        result["deployment_id_for_cleanup"] = deployment_id
        
        # Auto-cleanup if not using existing deployment
        if not existing_deployment_id:
            logger.info(f"🧹 Auto-cleaning up Docker container: {deployment_id}")
            cleanup_success = await cleanup_workflow_docker(deployment_id)
            result["auto_cleanup_success"] = cleanup_success
        
        return result
        
    except Exception as e:
        logger.error(f"Simplified exploit workflow failed: {e}")
        
        # Clean up Docker container if it was created
        if 'deployment_id' in locals():
            try:
                await cleanup_workflow_docker(deployment_id)
            except Exception as cleanup_error:
                logger.error(f"Failed to cleanup after workflow error: {cleanup_error}")
        
        return {
            "success": False,
            "workflow_id": workflow_id,
            "error": str(e),
            "target_path": target_path,
            "simplified_workflow": True
        }


async def cleanup_workflow_docker(deployment_id: str) -> bool:
    """
    Clean up Docker container used in workflow.
    
    Args:
        deployment_id: Docker container deployment ID
        
    Returns:
        bool: True if cleanup successful
    """
    try:
        logger.info(f"Cleaning up Docker container: {deployment_id}")
        success = await stop_docker_container(deployment_id)
        
        if success:
            logger.info(f"✅ Successfully cleaned up container: {deployment_id}")
        else:
            logger.warning(f"⚠️ Failed to clean up container: {deployment_id}")
        
        return success
        
    except Exception as e:
        logger.error(f"❌ Error cleaning up container {deployment_id}: {e}")
        return False


async def get_workflow_status(workflow_id: str) -> Dict[str, Any]:
    """
    Get workflow status - simplified version.
    """
    return {
        "workflow_id": workflow_id,
        "status": "completed",  # Simplified - always completed
        "message": "Simplified workflow engine - check logs for details"
    }


# Simple project analysis
async def analyze_project_simple(target_path: str) -> Dict[str, Any]:
    """
    Simple project analysis without complex LLM processing.
    """
    try:
        project_path = Path(target_path)
        
        # Basic file detection
        python_files = list(project_path.glob("**/*.py"))
        config_files = []
        
        # Look for common config files
        common_configs = ["requirements.txt", "pyproject.toml", "setup.py", "README.md"]
        for config in common_configs:
            config_path = project_path / config
            if config_path.exists():
                config_files.append(str(config_path))
        
        return {
            "success": True,
            "project_type": "python" if python_files else "unknown",
            "python_files_count": len(python_files),
            "config_files": config_files,
            "recommended_image": "python:3.12",
            "analysis_method": "simple_detection"
        }
        
    except Exception as e:
        logger.error(f"Simple project analysis failed: {e}")
        return {
            "success": False,
            "error": str(e),
            "recommended_image": "python:3.12"
        }



# Export key functions
__all__ = [
    'execute_optimized_exploit_workflow',
    'get_workflow_status', 
    'analyze_project_simple'
]