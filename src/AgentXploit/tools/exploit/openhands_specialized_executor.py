"""
Simplified OpenHands Specialized Executor
Minimal implementation focused on core task execution and trace collection
"""

import asyncio
import json
import os
import time
from dataclasses import dataclass
from pathlib import Path
from typing import Any, Dict, List, Optional

import logging

# Setup logger
logger = logging.getLogger(__name__)
if not logger.handlers:
    handler = logging.StreamHandler()
    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
    handler.setFormatter(formatter)
    logger.addHandler(handler)
    logger.setLevel(logging.INFO)


@dataclass
class OpenHandsConfig:
    """Configuration for OpenHands execution"""
    workspace_dir: str = "/workspace"
    openhands_dir: str = "/workspace"  # Run directly in /workspace
    shared_dir: str = "/shared"  # Use root-level /shared to match config.toml
    trajectories_path: str = "/shared/trajectories"  # Final trajectory file location (no extension)


class OpenHandsSpecializedExecutor:
    """
    Simplified OpenHands executor with minimal command execution
    Only executes user-provided or model-generated task commands
    """
    
    def __init__(self, deployment_id: str, deployment_config: Dict[str, Any] = None):
        """Initialize with deployment configuration"""
        self.deployment_id = deployment_id
        self.deployment_config = deployment_config or {}
        self.config = OpenHandsConfig()
        
        # Store target path if provided in config
        self.target_path = Path(deployment_config.get('target_path', '.')) if deployment_config else Path('.')
        
        logger.info(f"Initialized OpenHands executor for deployment: {deployment_id}")
        logger.info(f"Target path: {self.target_path}")
        logger.info(f"Trajectories will be saved to: {self.config.trajectories_path}")
    
    async def setup_minimal_environment(self) -> bool:
        """
        Minimal environment setup - only create workspace and copy essential files
        Do NOT touch /shared folder - it's already initialized
        """
        try:
            from .subprocess_docker import execute_in_container_session
            
            session_id = "openhands_session"
            
            # Only create workspace directory - /shared is already initialized
            setup_cmd = f"""
mkdir -p {self.config.workspace_dir} && \\
echo "Workspace directory created successfully"
"""
            
            result = await execute_in_container_session(
                command=setup_cmd.strip(),
                session_name=session_id,
                deployment_id=self.deployment_id,
                timeout=30.0
            )
            
            if not result.success:
                logger.error(f"Failed to create workspace directory: {result.stderr}")
                return False
            
            # Copy essential files (test_run.py is required)
            copy_ok = await self._copy_essential_files(session_id)
            if not copy_ok:
                logger.error("Aborting environment setup due to essential file copy failure")
                return False
            
            logger.info("Minimal environment setup completed")
            return True
            
        except Exception as e:
            logger.error(f"Environment setup failed: {e}")
            return False
    
    async def _copy_essential_files(self, session_id: str) -> bool:
        """Copy essential files - ALL files MUST be copied from local.
        Returns True on success, False if any file copy fails.
        """
        from .subprocess_docker import execute_in_container_session
        
        # Essential files to check and copy (order matters!)
        essential_files = ["test_run.py", "config.toml", "test.json"]
        
        logger.info(f"Copying essential files from {self.target_path} to container")
        
        for filename in essential_files:
            container_path = f"{self.config.openhands_dir}/{filename}"
            local_file = self.target_path / filename
            
            # All files MUST be copied from local, no fallback allowed
            logger.info(f"Copying {filename}")
            
            if not local_file.exists():
                logger.error(f"Local {filename} not found at {local_file}")
                return False
            
            # Copy file directly using docker cp command
            try:
                from .subprocess_docker import execute_raw_command
                
                # Use docker cp to copy file directly into container
                container_info = await self._get_container_id()
                if not container_info:
                    logger.error(f"Could not get container ID for deployment {self.deployment_id}")
                    return False
                
                container_id = container_info['container_id']
                copy_cmd = f"docker cp {local_file} {container_id}:{container_path}"
                
                copy_result = await execute_raw_command(copy_cmd)
                
                if copy_result.success:
                    logger.info(f"Copied {filename}")
                else:
                    logger.error(f"Failed to copy {filename}: {copy_result.stderr}")
                    return False
                    
            except Exception as e:
                logger.error(f"Error copying {filename}: {e}")
                return False
            
            # Add execute permissions for test_run.py
            if filename == "test_run.py":
                chmod_result = await execute_in_container_session(
                    command=f"chmod +x {container_path}",
                    session_name=session_id,
                    deployment_id=self.deployment_id,
                    timeout=10.0
                )
                
                if not chmod_result.success:
                    logger.warning(f"Failed to add execute permissions: {chmod_result.stderr}")
        
        # Verify files are accessible
        await self._verify_files_in_container(session_id)

        return True
    
    async def _get_container_id(self) -> Optional[Dict[str, Any]]:
        """Get container information for the current deployment"""
        from .subprocess_docker import _docker_runner
        return _docker_runner.get_container_info(self.deployment_id)
    
    async def _verify_files_in_container(self, session_id: str):
        """Verify that essential files exist and are readable in container"""
        from .subprocess_docker import execute_in_container_session
        
        verify_cmd = f"ls -1 {self.config.openhands_dir}/ | wc -l"
        result = await execute_in_container_session(
            command=verify_cmd,
            session_name=session_id,
            deployment_id=self.deployment_id,
            timeout=10.0
        )
        
        if result.success:
            file_count = result.stdout.strip()
            logger.info(f"Container files verified: {file_count} files copied")
        else:
            logger.warning(f"Could not verify container files: {result.stderr}")
    
    
    async def execute_task(
        self,
        task_description: str = "Analyze repository for security vulnerabilities",
        max_iterations: int = 10,
        custom_command: str = None
    ) -> Dict[str, Any]:
        """
        Execute OpenHands task - runs test_run.py or custom command
        If traces are not found after execution, terminate the workflow
        """
        try:
            from .subprocess_docker import execute_in_container_session
            
            session_id = "openhands_session"
            
            # Use custom command or default
            if custom_command:
                logger.info(f"Executing custom command: {custom_command}")
                full_command = custom_command
            else:
                logger.info("Executing default OpenHands test_run.py")
                # Ensure we're in the right directory and file has execute permissions
                full_command = "cd /workspace && chmod +x test_run.py && python3 test_run.py"
            
            # Execute the main command
            result = await execute_in_container_session(
                command=full_command,
                session_name=session_id,
                deployment_id=self.deployment_id,
                timeout=600.0
            )
            
            logger.info(f"Task execution completed. Success: {result.success}")
            
            # Print command results to terminal for user visibility
            print(f"\nüîß OpenHands Command Execution:")
            print(f"Command: {full_command}")
            print(f"Success: {'‚úÖ' if result.success else '‚ùå'}")
            
            if result.stdout:
                print(f"\nüì§ Output:")
                print(result.stdout)
            
            if result.stderr:
                print(f"\n‚ö†Ô∏è Errors/Warnings:")
                print(result.stderr)
            
            if not result.success:
                print(f"\n‚ùå Command failed with return code: {getattr(result, 'return_code', 'unknown')}")
                return {
                    "success": False,
                    "command": full_command,
                    "stdout": result.stdout,
                    "stderr": result.stderr,
                    "task_description": task_description,
                    "traces_found": 0,
                    "trace_files": [],
                    "error": "Command execution failed"
                }
            
            # CRITICAL: Check for traces after successful execution
            traces = await self._collect_traces()
            
            if not traces:
                error_msg = "No trajectories found after execution - terminating workflow"
                logger.error(f"‚ùå {error_msg}")
                print(f"\n‚ùå WORKFLOW TERMINATED: {error_msg}")
                print(f"Expected trajectories at: {self.config.trajectories_path}")
                print("Please check if test_run.py is correctly configured to save trajectories to /shared")
                
                return {
                    "success": False,  # Mark as failed to terminate workflow
                    "command": full_command,
                    "stdout": result.stdout,
                    "stderr": result.stderr,
                    "task_description": task_description,
                    "traces_found": 0,
                    "trace_files": [],
                    "error": error_msg,
                    "terminate_workflow": True  # Signal to terminate
                }
            
            return {
                "success": True,
                "command": full_command,
                "stdout": result.stdout,
                "stderr": result.stderr,
                "task_description": task_description,
                "traces_found": len(traces),
                "trace_files": traces,
                "trajectories_path": self.config.trajectories_path
            }
            
        except Exception as e:
            logger.error(f"Task execution failed: {e}")
            
            # Print error to terminal for user visibility
            print(f"\n‚ùå OpenHands Task Execution Failed:")
            print(f"Error: {str(e)}")
            print(f"Command: {custom_command or 'python3 test_run.py'}")
            print(f"Task: {task_description}")
            
            return {
                "success": False,
                "error": str(e),
                "command": custom_command or "python3 test_run.py",
                "task_description": task_description,
                "traces_found": 0,
                "trace_files": [],
                "terminate_workflow": True  # Signal to terminate on exception
            }
    
    async def _collect_traces(self) -> List[str]:
        """
        Collect trace files from /shared/trajectories - DO NOT modify /shared
        Just read what's already there after agent execution
        """
        try:
            from .subprocess_docker import execute_in_container_session
            
            session_id = "openhands_session"
            all_traces = []
            
            # Check if /shared/trajectories exists (could be file or directory)
            logger.info(f"Looking for trajectories at: {self.config.trajectories_path}")
            check_cmd = f"test -e {self.config.trajectories_path} && echo 'EXISTS' || echo 'NOT_FOUND'"
            
            result = await execute_in_container_session(
                command=check_cmd,
                session_name=session_id,
                deployment_id=self.deployment_id,
                timeout=10.0
            )
            
            if result.success and "EXISTS" in result.stdout:
                logger.info(f"Found trajectories at: {self.config.trajectories_path}")
                
                # Check if it's a file or directory
                type_cmd = f"test -f {self.config.trajectories_path} && echo 'FILE' || (test -d {self.config.trajectories_path} && echo 'DIRECTORY' || echo 'OTHER')"
                type_result = await execute_in_container_session(
                    command=type_cmd,
                    session_name=session_id,
                    deployment_id=self.deployment_id,
                    timeout=10.0
                )
                
                if type_result.success:
                    trace_type = type_result.stdout.strip()
                    logger.info(f"Trajectories type: {trace_type}")
                    
                    if "FILE" in trace_type:
                        # Single trajectory file
                        all_traces.append(self.config.trajectories_path)
                        logger.info(f"Found trajectory file: {self.config.trajectories_path}")
                        
                    elif "DIRECTORY" in trace_type:
                        # Directory with multiple trajectory files
                        all_traces.append(self.config.trajectories_path)
                        
                        # Count files in directory
                        count_cmd = f"ls -1 {self.config.trajectories_path}/ 2>/dev/null | wc -l"
                        count_result = await execute_in_container_session(
                            command=count_cmd,
                            session_name=session_id,
                            deployment_id=self.deployment_id,
                            timeout=10.0
                        )
                        if count_result.success:
                            file_count = count_result.stdout.strip()
                            logger.info(f"Directory contains {file_count} files")
                            
                            # Find JSON files in the directory
                            find_cmd = f"find {self.config.trajectories_path} -name '*.json' -type f 2>/dev/null"
                            find_result = await execute_in_container_session(
                                command=find_cmd,
                                session_name=session_id,
                                deployment_id=self.deployment_id,
                                timeout=10.0
                            )
                            if find_result.success and find_result.stdout.strip():
                                json_files = [f.strip() for f in find_result.stdout.strip().split('\n') if f.strip()]
                                all_traces.extend(json_files)
                                logger.info(f"Found {len(json_files)} JSON files in trajectories directory")
            else:
                logger.warning(f"No trajectories found at: {self.config.trajectories_path}")
                logger.warning("This will cause workflow termination")
            
            if not all_traces:
                logger.error("No trajectory files found - agent may not have completed successfully")
            else:
                logger.info(f"Total trace files found: {len(all_traces)}")
                for i, trace_file in enumerate(all_traces, 1):
                    logger.info(f"  {i}. {trace_file}")
            
            return all_traces
            
        except Exception as e:
            logger.error(f"Failed to collect traces: {e}")
            return []
    
    async def analyze_traces(self, trace_files: List[str]) -> Dict[str, Any]:
        """
        Simplified trace analysis with minimal file operations
        Only reads essential content for analysis
        """
        if not trace_files:
            logger.error("No trace files provided for analysis")
            return {
                "success": False,
                "error": "No trace files found",
                "injection_points": [],
                "external_data_sources": []
            }
        
        logger.info(f"Analyzing {len(trace_files)} trace files")
        
        injection_points = []
        external_sources = []
        
        try:
            from .subprocess_docker import execute_in_container_session
            
            session_id = "openhands_session"
            
            # Analyze maximum 3 files to avoid overhead
            for trace_file in trace_files[:3]:
                try:
                    # Read only first 50 lines for analysis
                    read_cmd = f"head -50 '{trace_file}'"
                    
                    result = await execute_in_container_session(
                        command=read_cmd,
                        session_name=session_id,
                        deployment_id=self.deployment_id,
                        timeout=10.0
                    )
                    
                    if result.success and result.stdout:
                        content = result.stdout.lower()
                        
                        # Simple pattern matching for injection detection
                        if any(pattern in content for pattern in ['command', 'exec', 'api', 'github']):
                            injection_points.append({
                                "type": "openhands_execution",
                                "location": trace_file,
                                "description": "OpenHands execution trace",
                                "risk": "MEDIUM",
                                "agent_type": "openhands"
                            })
                        
                        # Detect external input sources
                        if any(pattern in content for pattern in ['task', 'prompt', 'input']):
                            external_sources.append({
                                "source": "task_input",
                                "location": trace_file,
                                "content": f"Trace: {os.path.basename(trace_file)}",
                                "risk": "MEDIUM"
                            })
                            
                except Exception as e:
                    logger.warning(f"Failed to analyze {trace_file}: {e}")
            
            return {
                "success": True,
                "total_traces": len(trace_files),
                "injection_points": injection_points,
                "external_data_sources": external_sources,
                "high_risk_points": [p for p in injection_points if p.get("risk") == "HIGH"],
                "overall_risk": "HIGH" if any(p.get("risk") == "HIGH" for p in injection_points) else "MEDIUM"
            }
            
        except Exception as e:
            logger.error(f"Trace analysis failed: {e}")
            return {
                "success": False,
                "error": str(e),
                "injection_points": [],
                "external_data_sources": []
            }
    
    async def cleanup(self):
        """
        Minimal cleanup - only remove temporary files if needed
        Do NOT touch /shared folder
        """
        try:
            logger.info("Cleanup completed (no operations needed)")
        except Exception as e:
            logger.error(f"Cleanup failed: {e}")


# Global instance for backward compatibility
_openhands_executor = None

def get_openhands_executor(deployment_id: str, config: Dict[str, Any] = None) -> OpenHandsSpecializedExecutor:
    """Get or create OpenHands executor instance"""
    global _openhands_executor
    
    if _openhands_executor is None or _openhands_executor.deployment_id != deployment_id:
        _openhands_executor = OpenHandsSpecializedExecutor(deployment_id, config)
    
    return _openhands_executor


# Backward compatibility functions
async def setup_openhands_environment(deployment_id: str, config: Dict[str, Any] = None) -> bool:
    """Setup OpenHands environment with minimal configuration"""
    executor = get_openhands_executor(deployment_id, config)
    return await executor.setup_minimal_environment()


async def execute_openhands_task(
    deployment_id: str,
    task_description: str = "Analyze repository for security vulnerabilities",
    max_iterations: int = 10,
    custom_command: str = None
) -> Dict[str, Any]:
    """Execute OpenHands task with simplified logic"""
    executor = get_openhands_executor(deployment_id)
    return await executor.execute_task(task_description, max_iterations, custom_command)


async def analyze_openhands_traces(deployment_id: str, trace_files: List[str] = None) -> Dict[str, Any]:
    """Analyze OpenHands traces with minimal operations"""
    executor = get_openhands_executor(deployment_id)
    
    if trace_files is None:
        trace_files = await executor._collect_traces()
    
    return await executor.analyze_traces(trace_files)